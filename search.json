[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Prof. Giuseppe Ragusa\n   Sapienza, Università di Roma\n   Dipartimento di Economia e Diritto\n   Prof. Giuseppe Ragusa\n   giuseppe.ragusa@uniroma1.it\n   Ricevimento\n\n\n\n\n\n   Lunedì (L) e Martedì (M)\n   19 febbraio, 2024 - 1 giugno 2024\n   (L) 10:00–12:00 (M) 14:00-16:00\n   (L) Aula 10 (M) Aula 8b"
  },
  {
    "objectID": "syllabus.html#descrizione-del-corso",
    "href": "syllabus.html#descrizione-del-corso",
    "title": "Syllabus",
    "section": "Descrizione del corso",
    "text": "Descrizione del corso\nEconometria (1018133) offre agli studenti un’introduzione pratica all’econometria, strumento fondamentale per comprendere e analizzare l’economia da un punto di vista empirico. Attraverso un approccio equilibrato tra teoria e pratica, gli studenti esploreranno i concetti fondamentali e gli strumenti empirici necessari per comprendere le sfide relative all’impiego di tecniche quantitative in microeconomia, macroeconomia e finanza.\nL’enfasi è sulla comprensione intuitiva e non sulle formalità matematiche questo corso ti fornirà le competenze necessarie per affrontare sfide analitiche complesse con sicurezza e competenza."
  },
  {
    "objectID": "syllabus.html#obiettivi-del-corso",
    "href": "syllabus.html#obiettivi-del-corso",
    "title": "Syllabus",
    "section": "Obiettivi del corso",
    "text": "Obiettivi del corso\nAlla conclusione del corso, gli studenti saranno in grado di condurre analisi empiriche in modo indipendente e interpretarne i risultati, valutando attentamente l’adeguatezza delle assunzioni necessarie per garantire la corretta interpretazione dei risultati.\nIn particolare, gli studenti saranno in grado di:\n\nComprendere la logica e la filosofia dei modelli econometrici presentati, valutando la loro capacità di cogliere relazioni causali o di fornire predizioni di qualità.\nValutare criticamente le assunzioni fondamentali sottostanti ai modelli econometrici, analizzandone l’impatto sulla validità dei risultati.\nUtilizzare rigorose misure statistiche per valutare la qualità delle predizioni generate dai modelli econometrici.\nApplicare efficacemente il software R per manipolare dati, produrre visualizzazioni informative e stimare i parametri dei modelli econometrici utilizzando dati reali, sviluppando così competenze pratiche nel campo dell’analisi dei dati e della modellistica economica."
  },
  {
    "objectID": "syllabus.html#libro-di-testo",
    "href": "syllabus.html#libro-di-testo",
    "title": "Syllabus",
    "section": "Libro di testo",
    "text": "Libro di testo\n\n\n\n\n\nIl libro di testo utilizzato in questo corso è:\nStock, J. H. e Watson, M.W: Introduzione all’econometria, Pearson Italia, 2020. ISBN: 8891906190.\nDurante il corso, gli studenti avranno accesso a slides didattiche che forniranno una guida dettagliata attraverso i concetti e gli argomenti trattati. Queste risorse supplementari saranno fondamentali per consolidare la comprensione dei materiali di studio e facilitare il processo di apprendimento."
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Syllabus",
    "section": "Software",
    "text": "Software\n\n\n\n\n\nCome già accennato, il corso prevede l’utilizzo di R, uno dei linguaggi di programmazione statistico/econometrico più diffusi e potenti.\nR ha una licenza open-source (GNU GPL), è compatibile con i maggiori sistemi operativi (GNU/Linux, macOS, Microsoft Windows)."
  },
  {
    "objectID": "syllabus.html#orari-di-ricevimento",
    "href": "syllabus.html#orari-di-ricevimento",
    "title": "Syllabus",
    "section": "Orari di ricevimento",
    "text": "Orari di ricevimento\nGli orari di ricevimento sono:\n\n\n\nGiorno\nOrario\n\n\n\n\nLunedì\n9:00-10:00\n\n\nMartedì\n9:00-10:00\n\n\n\nE’ obbligatorio prenotare un appuntamento a questo link."
  },
  {
    "objectID": "syllabus.html#modalità-di-valutazione",
    "href": "syllabus.html#modalità-di-valutazione",
    "title": "Syllabus",
    "section": "Modalità di valutazione",
    "text": "Modalità di valutazione\nL’esame di Econometria comprende due fasi: una prova scritta e una prova orale, entrambe obbligatorie.\nLa prova scritta valuterà la capacità degli studenti di comprendere e interpretare le stime dei modelli econometrici introdotti durante il corso e di applicarli in contesti specifici.\nLa prova orale è volta ad accertare in maniera piu’ articolata le conoscenza acquisite nel corso.\nL’ammissione alla prova orale è garantita agli studenti che abbiano superato con sufficienza la prova scritta."
  },
  {
    "objectID": "panel_app.html",
    "href": "panel_app.html",
    "title": "Panel data",
    "section": "",
    "text": "Il modello \\[\ny_{it} = \\alpha_i + \\lambda_t + \\beta_1X_{1it} + \\dots + \\beta_k X_{kit} + u_{it},\n\\] può essere stimato tramite il metodo dei minimi quadrati ordinari (OLS), includendo le dummy per ciascuno stato e per ciascun periodo. Alternativamente, possiamo stimare il modello tramite OLS dopo che le variabili sono state trasformate mediante la trasformazione within.\nPer ottenere le stime dei coefficienti in entrambi gli approcci, che ricordiamo essere numericamente equivalenti, è possibile utilizzare il pacchetto fixest.\nUtilizziamo i dati nel dataset Fatility.\n\nlibrary(Ecdat)\ndata(\"Fatality\")\n\nEcco la traduzione in italiano del testo fornito:\nFatality consiste di 336 osservazioni su 10 variabili. Queste variabili riguardano la mortalità stradale, le tasse sull’alcol, e le leggi sulla guida in stato di ebbrezza in 48 stati degli Stati Uniti dal 1982 al 1988. Si noti che state è una variabile che prende 48 valori distinti (uno per ciascuno dei 48 stati contigui federali degli USA). Anche year prende 7 valori distinti che identificando il periodo temporale in cui è stata fatta l’osservazione. Questo ci dà un totale di 7×48=336 osservazioni. Poiché tutte le variabili sono osservate per tutte le entità e in tutti i periodi di tempo, il pannello è bilanciato. Se ci fossero dati mancanti per almeno un’entità in almeno un periodo di tempo, chiameremmo il pannello non bilanciato.\nLe variabili contenute nel dataset Fatality sono:\n\n\n\nVariabile\nDescrizione\n\n\n\n\nstate\nCodice identificativo dello stato\n\n\nyear\nAnno\n\n\nmrall\nTasso di mortalità stradale (morti per 10.000)\n\n\nbeertax\nTassa su una cassa di birra\n\n\nmlda\nEtà minima legale per il consumo di alcol\n\n\njaild\nPresenza di pene detentive obbligatorie?\n\n\ncomserd\nObbligatorietà del servizio comunitario?\n\n\nvmiles\nMiglia medie percorse per conducente\n\n\nunrate\nTasso di disoccupazione\n\n\nperinc\nReddito personale per capita",
    "crumbs": [
      "Syllabus",
      "R",
      "Panel data"
    ]
  },
  {
    "objectID": "panel_app.html#dati-panel",
    "href": "panel_app.html#dati-panel",
    "title": "Panel data",
    "section": "",
    "text": "Il modello \\[\ny_{it} = \\alpha_i + \\lambda_t + \\beta_1X_{1it} + \\dots + \\beta_k X_{kit} + u_{it},\n\\] può essere stimato tramite il metodo dei minimi quadrati ordinari (OLS), includendo le dummy per ciascuno stato e per ciascun periodo. Alternativamente, possiamo stimare il modello tramite OLS dopo che le variabili sono state trasformate mediante la trasformazione within.\nPer ottenere le stime dei coefficienti in entrambi gli approcci, che ricordiamo essere numericamente equivalenti, è possibile utilizzare il pacchetto fixest.\nUtilizziamo i dati nel dataset Fatility.\n\nlibrary(Ecdat)\ndata(\"Fatality\")\n\nEcco la traduzione in italiano del testo fornito:\nFatality consiste di 336 osservazioni su 10 variabili. Queste variabili riguardano la mortalità stradale, le tasse sull’alcol, e le leggi sulla guida in stato di ebbrezza in 48 stati degli Stati Uniti dal 1982 al 1988. Si noti che state è una variabile che prende 48 valori distinti (uno per ciascuno dei 48 stati contigui federali degli USA). Anche year prende 7 valori distinti che identificando il periodo temporale in cui è stata fatta l’osservazione. Questo ci dà un totale di 7×48=336 osservazioni. Poiché tutte le variabili sono osservate per tutte le entità e in tutti i periodi di tempo, il pannello è bilanciato. Se ci fossero dati mancanti per almeno un’entità in almeno un periodo di tempo, chiameremmo il pannello non bilanciato.\nLe variabili contenute nel dataset Fatality sono:\n\n\n\nVariabile\nDescrizione\n\n\n\n\nstate\nCodice identificativo dello stato\n\n\nyear\nAnno\n\n\nmrall\nTasso di mortalità stradale (morti per 10.000)\n\n\nbeertax\nTassa su una cassa di birra\n\n\nmlda\nEtà minima legale per il consumo di alcol\n\n\njaild\nPresenza di pene detentive obbligatorie?\n\n\ncomserd\nObbligatorietà del servizio comunitario?\n\n\nvmiles\nMiglia medie percorse per conducente\n\n\nunrate\nTasso di disoccupazione\n\n\nperinc\nReddito personale per capita",
    "crumbs": [
      "Syllabus",
      "R",
      "Panel data"
    ]
  },
  {
    "objectID": "panel_app.html#modello-senza-effetti-fissi-temporali",
    "href": "panel_app.html#modello-senza-effetti-fissi-temporali",
    "title": "Panel data",
    "section": "Modello senza effetti fissi temporali",
    "text": "Modello senza effetti fissi temporali\nIl primo modello che consideriamo è \\[\nmrall_{it} = \\alpha_i + \\beta_1beertax_{it} + u_{it}\n\\]\n\nlibrary(ggplot2)\nggplot(Fatality, aes(y=mrall, x = beertax)) +\n  geom_point(aes(col=factor(state))) + \n  geom_smooth(method='lm', col = \"black\") +\n  geom_smooth(aes(col=factor(state)), method='lm', se = FALSE) +\n  theme_minimal() + \n  theme(legend.position = 'none')\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nFigure 1 esplora la relazione tra la tassa sulla birra (beertax) e il tasso di mortalità stradale (mrall), con un’attenzione particolare alle differenze tra stati. Nel grafico sono mostrate le rette di regressione per ciascuno stato (colori distinti per i punti e per le rette di regressione indicano uno stato). Come si vede, molte delle rette hanno un’inclinazione negativa. Comunque, se considerassimo un’unica regressione otterremo un coefficiente positivo come indicato dalla retta di regressione di colore nero. Tenere conto degli effetti fissi, cioè di \\(\\alpha_i\\), permetteremo alle singolo rette di regressione di avere intercette distinte.\n\nlibrary(fixest)\npd0 &lt;- feols(mrall~beertax, data=Fatality, vcov = \"hetero\")\npd1 &lt;- feols(mrall~beertax|state, data=Fatality, vcov = \"hetero\")\npd2 &lt;- feols(mrall~beertax|state, data=Fatality, cluster = ~state)\n\n\n\n\nIl coefficiente di BeerTax, \\(\\hat{\\beta}_1\\), è negativo e statisticamente significativo. L’interpretazione è che la riduzione stimata delle fatalità stradali dovuta a un aumento di $1 nella tassa sulla birra è di 0,66 per 10000 persone o, di 66 persone ogni milione di abitanti.\nSebbene l’inclusione degli effetti fissi statali elimini il rischio di una distorsione dovuta a fattori omessi che variano tra gli stati ma non nel tempo, è possibile che ci siano altre variabili omesse che variano nel tempo ma non fra stati. Possiamo tenere conto di queste variabili aggiungendo gli effetti temporali.\n\nlibrary(fixest)\npd3 &lt;- feols(mrall~beertax|state+year, data=Fatality, vcov = \"hetero\")\npd4 &lt;- feols(mrall~beertax|state+year, data=Fatality, cluster = ~state)\n\n\n\n\nIl coefficiente stimato in un modello con gli effetti fissi per anno è -0.66. È molto simile al coefficiente stimato per il modello di regressione che include solo gli effetti fissi dell’entità, ma adesso è stimato con minore precisione ed è statisticamente significativo al 10% quando usiamo gli errori standard che tengono conto di potenziale correlazione temporale nei residui (cluster(year)).\nPossiamo concludere che la relazione stimata tra le fatalità stradali e la tassa sulla birra non è influenzata da variabili omesse a causa di fattori che sono costanti nel tempo o tra gli stati.\nMa ci sono ancora due fonti di distorsione dovuta a variabili omesse che non sono state considerate da tutti i modelli usati: le condizioni economiche e le leggi sulla guida. I dati a nostra disposizione includono informazioni specifiche per stato sull’età legale per bere (mlda), le pene (jaild, comserd) e vari indicatori economici come il tasso di disoccupazione (unrate) e il reddito pro capite (perinc). Possiamo utilizzare queste covariate per estendere l’analisi precedente.\nIl modello che consideriamo adesso è\n\\[\n\\begin{aligned}\nmrall_{it} = \\alpha_i &+ \\beta_1 beertax_{it} + \\beta_2 unrate_{it} + \\beta_3 \\log(perinc_{it}) + \\beta_4 mlda_{it} \\\\\n&+ \\beta_5 comserd_{it} + \\beta_6 jaild_{it} + u_{it}\n\\end{aligned}\n\\tag{1}\\]\nStimiamo il modello Equation 1 includendo una variabile dummy per ciascuno stato (tranne uno per evitare multicollinearità), utilizzando feols e aggiungendo factor(state) alla formula:\n\nlm1_dummy &lt;- feols(mrall ~ beertax + unrate + log(perinc) + mlda + comserd + jaild \n                   + factor(state), data=Fatality, vcov = \"hetero\")\nlm1_dummy\n\nOLS estimation, Dep. Var.: mrall\nObservations: 336 \nStandard-errors: Heteroskedasticity-robust \n                Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)    -2.239669   3.732728 -0.600009 0.548982    \nbeertax        -0.359256   0.221616 -1.621075 0.106119    \nunrate         -0.019057   0.009784 -1.947822 0.052428 .  \nlog(perinc)     0.662102   0.385591  1.717111 0.087057 .  \nmlda           -0.037538   0.019161 -1.959105 0.051086 .  \ncomserdyes     -0.019369   0.118064 -0.164052 0.869808    \njaildyes       -0.019841   0.032254 -0.615143 0.538956    \nfactor(state)4 -0.309122   0.349763 -0.883802 0.377556    \n... 46 coefficients remaining (display them with summary() or use argument n)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.167474   Adj. R2: 0.897213\n\n\nAlternativamente, possiamo applicare la trasformazione within, ovvero eseguire la regressione sulle variabili trasformate in deviazione dalla media temporale di ciascuna variabile, aggiungendo |state alla fine della formula:\n\nlm_fe &lt;- feols(mrall ~ beertax + unrate + log(perinc) + mlda + comserd \n               + jaild | state, data=Fatality)\nlm_fe\n\nOLS estimation, Dep. Var.: mrall\nObservations: 336 \nFixed-effects: state: 48\nStandard-errors: Clustered (state) \n             Estimate Std. Error   t value Pr(&gt;|t|)    \nbeertax     -0.359256   0.294458 -1.220058 0.228530    \nunrate      -0.019057   0.012734 -1.496547 0.141197    \nlog(perinc)  0.662102   0.547510  1.209296 0.232597    \nmlda        -0.037538   0.025577 -1.467660 0.148857    \ncomserdyes  -0.019369   0.123518 -0.156807 0.876068    \njaildyes    -0.019841   0.007608 -2.607756 0.012178 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.167474     Adj. R2: 0.897213\n                 Within R2: 0.126181\n\n\nI coefficienti stimati in lm_dummy sono identici a quelli in lm_fe, tuttavia gli errori standard differiscono. In lm_fe, feols utilizza automaticamente errori standard clusterizzati per stato.\n\nlm_dummy_cluster &lt;- feols(mrall ~ beertax + unrate + log(perinc) + mlda + comserd \n                          + jaild + factor(state), data=Fatality, cluster=\"state\")\nlm_dummy_cluster\n\nOLS estimation, Dep. Var.: mrall\nObservations: 336 \nStandard-errors: Clustered (state) \n                Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)    -2.239669   5.727427 -0.391043 0.697533    \nbeertax        -0.359256   0.318051 -1.129554 0.264396    \nunrate         -0.019057   0.013754 -1.385533 0.172430    \nlog(perinc)     0.662102   0.591379  1.119591 0.268577    \nmlda           -0.037538   0.027626 -1.358789 0.180699    \ncomserdyes     -0.019369   0.133415 -0.145175 0.885193    \njaildyes       -0.019841   0.008218 -2.414313 0.019712 *  \nfactor(state)4 -0.309122   0.466877 -0.662105 0.511138    \n... 46 coefficients remaining (display them with summary() or use argument n)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.167474   Adj. R2: 0.897213\n\n\nGli errori standard in lm_dummy_cluster e lm_fe presentano lievi differenze dovute alla gestione diversa dei gradi di libertà nei due metodi.",
    "crumbs": [
      "Syllabus",
      "R",
      "Panel data"
    ]
  },
  {
    "objectID": "panel_app.html#aggiungere-effetti-fissi-temporali",
    "href": "panel_app.html#aggiungere-effetti-fissi-temporali",
    "title": "Panel data",
    "section": "Aggiungere effetti fissi temporali",
    "text": "Aggiungere effetti fissi temporali\nÈ possibile includere effetti temporali aggiungendo year alla specificazione di feols:\n\nlm_fe &lt;- feols(mrall ~ beertax + unrate + log(perinc) + mlda + comserd \n               + jaild | state + year, data=Fatality)\nlm_fe\n\nOLS estimation, Dep. Var.: mrall\nObservations: 336 \nFixed-effects: state: 48,  year: 7\nStandard-errors: Clustered (state) \n             Estimate Std. Error   t value   Pr(&gt;|t|)    \nbeertax     -0.476567   0.303670 -1.569357 1.2327e-01    \nunrate      -0.062880   0.013034 -4.824134 1.5206e-05 ***\nlog(perinc)  1.796308   0.642674  2.795052 7.4898e-03 ** \nmlda        -0.001890   0.021519 -0.087826 9.3039e-01    \ncomserdyes   0.034492   0.132229  0.260853 7.9535e-01    \njaildyes     0.014597   0.016246  0.898519 3.7349e-01    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.140612     Adj. R2: 0.925966\n                 Within R2: 0.354415\n\n\nOppure, possiamo aggiungere variabili dummy per year nel modello:\n\nlm_dummy_year_cluster &lt;- feols(mrall ~ beertax + unrate + log(perinc) + mlda + comserd \n                               + jaild + factor(state) + factor(year), data=Fatality, cluster=\"state\")\nlm_dummy_year_cluster\n\nOLS estimation, Dep. Var.: mrall\nObservations: 336 \nStandard-errors: Clustered (state) \n                 Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)    -12.603660   6.784529 -1.857706 6.9482e-02 .  \nbeertax         -0.476567   0.328510 -1.450691 1.5351e-01    \nunrate          -0.062880   0.014101 -4.459361 5.0929e-05 ***\nlog(perinc)      1.796308   0.695244  2.583707 1.2944e-02 *  \nmlda            -0.001890   0.023279 -0.081185 9.3564e-01    \ncomserdyes       0.034492   0.143045  0.241129 8.1050e-01    \njaildyes         0.014597   0.017574  0.830578 4.1041e-01    \nfactor(state)4  -0.898339   0.481419 -1.866024 6.8285e-02 .  \n... 52 coefficients remaining (display them with summary() or use argument n)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.140612   Adj. R2: 0.925966\n\n\nIn entrambi i casi, gli errori standard considerano la correlazione temporale all’interno degli stati, ma differiscono leggermente a causa di come i gradi di libertà sono gestiti nei due approcci.",
    "crumbs": [
      "Syllabus",
      "R",
      "Panel data"
    ]
  },
  {
    "objectID": "info-esame.html",
    "href": "info-esame.html",
    "title": "Informazioni sull’Esame",
    "section": "",
    "text": "Date dell’Esame\nL’esame si terrà nelle seguenti date:\n\n20 Giugno 2024 (Primo appello)\n10 Luglio 2024 (Secondo appello)\n\n\n\nStruttura dell’Esame\nL’esame sarà composto da due prove:\n\nProva scritta su piattaforma Moodle.\nUna simulazione della prova scritta è disponibile sulla piattaforma Moodle. Altre domande (non in formato moodle ma utili per eserciatarsi sono disponili qua)\nProva orale\n\nPer poter accedere alla prova orale, è necessario ottenere almeno 18/30 nella prova scritta. Il voto finale sarà determinato dalla combinazione dei risultati delle due prove.\n\n\nProgramma d’Esame\nIl programma d’esame per il corso 2023/2024 copre i seguenti argomenti (corrispondenti ai primi 10 capitoli del libro di testo):\n\n\nIntroduzione all’Econometria\n\nIntroduzione e richiami (Capitolo 1)\n\nDomande economiche e dati economici\nEffetti causali ed esperimenti ideali\nTipi e fonti dei dati economici\n\n\n\n\nRichiami di Probabilità\n\nRichiami di probabilità (Capitolo 2)\n\nVariabili casuali e distribuzioni di probabilità\nValore atteso, media e varianza\nVariabili casuali doppie e distribuzioni congiunte\nDistribuzioni normale, chi-quadrato, F e t di Student\nCampionamento casuale e distribuzione della media campionaria\n\n\n\n\nRichiami di Statistica\n\nRichiami di statistica (Capitolo 3)\n\nStima della media di una popolazione\nVerifica di ipotesi circa la media della popolazione\nIntervalli di confidenza\nConfronto tra medie di popolazioni diverse\nDiagrammi a nuvola di punti, covarianza e correlazione campionaria\n\n\n\n\nRegressione Lineare Semplice\n\nElementi fondamentali della regressione (Capitolo 4)\n\nIl modello di regressione lineare\nStima dei coefficienti del modello di regressione lineare\nLe assunzioni dei minimi quadrati ordinari (OLS)\nDistribuzione campionaria degli stimatori OLS\nVerifica di ipotesi e intervalli di confidenza per un singolo coefficiente di regressione\n\n\n\n\nRegressione Lineare Multipla\n\nRegressione lineare con regressori multipli (Capitolo 5)\n\nDistorsione da variabile omessa\nIl modello di regressione multipla\nStima e assunzioni dei minimi quadrati\nDistribuzione campionaria degli stimatori OLS nella regressione multipla\nVerifica di ipotesi e intervalli di confidenza per più coefficienti\nStatistiche di regressione e altre misure di bontà di adattamento\n\n\n\n\nRegressioni Non Lineari\n\nFunzioni di regressione non lineari (Capitolo 6)\n\nModellare funzioni di regressione non lineari\nFunzioni non lineari di una singola variabile indipendente\nInterazioni tra variabili indipendenti\n\n\n\n\nValutazione di Studi Basati sulla Regressione\n\nValutazione di studi basati sulla regressione multipla (Capitolo 7)\n\nValidità interna ed esterna\nMinacce alla validità interna ed esterna\n\n\n\n\nRegressione con Dati Panel\n\nRegressione con dati panel (Capitolo 8)\n\nIntroduzione ai dati panel\nModelli di regressione con effetti fissi e temporali\n\n\n\n\nRegressione con Variabile Dipendente Binaria\n\nRegressione con variabile dipendente binaria (Capitolo 9)\n\nModello lineare di probabilità\nRegressioni probit e logit\nStima e inferenza nei modelli logit e probit\n\n\n\n\nRegressione con Variabili Strumentali\n\nRegressione con variabili strumentali (Capitolo 10)\n\nIl modello IV e le sue ipotesi\nLo stimatore dei minimi quadrati a due stadi (TSLS)\nValidità degli strumenti\nTest di esogeneità\nStrumenti deboli\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPer gli studenti degli anni precedenti (corso da 9 crediti), il programma include i seguenti argomenti:\n\n\nRegressioni per Serie Temporali di Tipo Economico\n\nIntroduzione a regressioni temporali e previsioni (Capitolo 12)\n\nL’uso dei modelli di regressione per la previsione\nIntroduzione alle serie temporali e alla correlazione seriale\nAutoregressioni e modelli autoregressivi misti\nScelta della lunghezza dei ritardi utilizzando i criteri d’informazione\nNon stazionarietà: trend e rotture strutturali\n\n\n\n\nStima degli Effetti Causali Dinamici\n\nStima degli effetti causali dinamici (Capitolo 13)\n\nEffetti causali e dati temporali\nStima degli effetti causali dinamici con regressori esogeni e strettamente esogeni\nErrori standard consistenti in presenza di autocorrelazione ed eteroschedasticità\n\n\n\n\nUlteriori Sviluppi nelle Regressioni Temporali\n\nUlteriori sviluppi nelle regressioni temporali (Capitolo 14)\n\nAutoregressioni vettoriali (VAR)\nPrevisioni multiperiodali\nCointegrazione\nEteroschedasticità condizionata"
  },
  {
    "objectID": "IntroduzioneR.html",
    "href": "IntroduzioneR.html",
    "title": "R: Introduzione",
    "section": "",
    "text": "R è un linguaggio di programmazione open source utilizzato per l’analisi e visualizzazione dei dati.\nR è la versione open source di S, software sviluppato da John Chambers, Rick Becker and Allan Wilks presso i Bell Laboratories di AT&T alla fine degli anni ’70. Nell’idea del suoi creatori, S doveva essere un linguaggio di programmazione che fosse più semplice e più interattivo rispetto ai linguaggi utilizzati per l’analisi dei dati dell’epoca (FORTRAN e SAS).\nR fu sviluppato inizialmente da Ross Ihaka e Robert Gentleman presso l’Università di Auckland, in Nuova Zelanda. La prima versione di R venne condivisa dai sui creatori nel 1993. Nel 1995 il software fu rilasciato sotto la licenza open source GNU GPL.\nLa versione 1.0 è stata rilasciata il 29 febbraio 2000 e da quel momento la crescita della sua popolarità non si è arrestata.\nNegli ultimi dieci anni, due aziende hanno contribuito alla crescita di R: R Studio (oggi diventata Posit) e Revolution Analytics.\nRevolution Analytics fondata nel 2007, è stata una delle prime società a offrire una versione commerciale di R. Revolution Analytics ha introdotto nuove funzionalità, come l’elaborazione parallela e distribuita, che hanno permesso di elaborare grandi quantità di dati in modo più efficiente. L’acquisizione di Revolution Analytics da parte di Microsoft nel 2015 ha ulteriormente rafforzato l’adozione di R, poiché Microsoft ha iniziato a integrare R in molti dei suoi prodotti, come SQL Server e Power BI.\nRStudio, dal 2022 Posit, ha avuto un ruolo fondamentale nello sviluppo dell’ecosistema R. Fondata nel 2011 da JJ Allaire, l’azienda ha introdotto un ambiente di sviluppo integrato (IDE) per R, che ha rivoluzionato il modo in cui gli utenti interagiscono con il linguaggio. L’ambiente integrato, anch’esso chiamato RStudio, ha reso la programmazione in R più accessibile e intuitiva, facilitando la scrittura, il debug e la gestione dei pacchetti.\nOltre al suo IDE, RStudio ha svolto un ruolo chiave nello sviluppo del “tidyverse”, una collezione di pacchetti R per la scienza dei dati, guidata da Hadley Wickham, uno dei più influenti progammatori di R e Chief Scientist di Posit. Il tidyverse, introdotto intorno al 2014, comprende pacchetti come ggplot2, dplyr e tidyr usati per la visualizzazione, la manipolazione e la ristrutturazione dei dati. Questi strumenti hanno reso più efficiente e meno soggette a errori l’analisi dei dati, promuovendo uno stile di programmazione coerente e leggibile.\nR è oggi uno dei linguaggi di programmazione più popolari per l’analisi dei dati e la statistica negli ultimi anni. Ecco alcuni dati sull’adozione di R:\n\nNumero di utenti: Secondo un sondaggio del 2022 condotto da Stack Overflow R è uno dei linguaggi di programmazione più popolare al mondo, utilizzato dal 4.7% degli sviluppatori. Nella lista è il primo software ad essere specificatamente pensato per applicazioni statistiche.\nNumero di pacchetti: Il Comprehensive R Archive Network (CRAN), il principale repository di pacchetti per R, contiene oltre 19.000 pacchetti. Questi pacchetti coprono una vasta gamma di applicazioni che vanno dalla statistica al machine learning, dalla visualizzazione e manipolazione dei dati alla stesura automatica di report, e altri task generici non necessariamente legati alle applicazioni di carattere statistico.\nUso aziendale: L’adozione di R nell’ambiente aziendale è aumentata negli ultimi anni, con molte aziende che lo utilizzano per l’analisi dei dati e la statistica.\n\nIn generale, R è diventato un linguaggio di programmazione popolare tra i data scientist, gli statistici e gli analisti dei dati grazie alla sua vasta gamma di pacchetti, alla sua flessibilità e alla sua potenza.\nQuore.",
    "crumbs": [
      "Syllabus",
      "R",
      "Introduzione a `R`"
    ]
  },
  {
    "objectID": "IntroduzioneR.html#storia",
    "href": "IntroduzioneR.html#storia",
    "title": "R: Introduzione",
    "section": "",
    "text": "R è un linguaggio di programmazione open source utilizzato per l’analisi e visualizzazione dei dati.\nR è la versione open source di S, software sviluppato da John Chambers, Rick Becker and Allan Wilks presso i Bell Laboratories di AT&T alla fine degli anni ’70. Nell’idea del suoi creatori, S doveva essere un linguaggio di programmazione che fosse più semplice e più interattivo rispetto ai linguaggi utilizzati per l’analisi dei dati dell’epoca (FORTRAN e SAS).\nR fu sviluppato inizialmente da Ross Ihaka e Robert Gentleman presso l’Università di Auckland, in Nuova Zelanda. La prima versione di R venne condivisa dai sui creatori nel 1993. Nel 1995 il software fu rilasciato sotto la licenza open source GNU GPL.\nLa versione 1.0 è stata rilasciata il 29 febbraio 2000 e da quel momento la crescita della sua popolarità non si è arrestata.\nNegli ultimi dieci anni, due aziende hanno contribuito alla crescita di R: R Studio (oggi diventata Posit) e Revolution Analytics.\nRevolution Analytics fondata nel 2007, è stata una delle prime società a offrire una versione commerciale di R. Revolution Analytics ha introdotto nuove funzionalità, come l’elaborazione parallela e distribuita, che hanno permesso di elaborare grandi quantità di dati in modo più efficiente. L’acquisizione di Revolution Analytics da parte di Microsoft nel 2015 ha ulteriormente rafforzato l’adozione di R, poiché Microsoft ha iniziato a integrare R in molti dei suoi prodotti, come SQL Server e Power BI.\nRStudio, dal 2022 Posit, ha avuto un ruolo fondamentale nello sviluppo dell’ecosistema R. Fondata nel 2011 da JJ Allaire, l’azienda ha introdotto un ambiente di sviluppo integrato (IDE) per R, che ha rivoluzionato il modo in cui gli utenti interagiscono con il linguaggio. L’ambiente integrato, anch’esso chiamato RStudio, ha reso la programmazione in R più accessibile e intuitiva, facilitando la scrittura, il debug e la gestione dei pacchetti.\nOltre al suo IDE, RStudio ha svolto un ruolo chiave nello sviluppo del “tidyverse”, una collezione di pacchetti R per la scienza dei dati, guidata da Hadley Wickham, uno dei più influenti progammatori di R e Chief Scientist di Posit. Il tidyverse, introdotto intorno al 2014, comprende pacchetti come ggplot2, dplyr e tidyr usati per la visualizzazione, la manipolazione e la ristrutturazione dei dati. Questi strumenti hanno reso più efficiente e meno soggette a errori l’analisi dei dati, promuovendo uno stile di programmazione coerente e leggibile.\nR è oggi uno dei linguaggi di programmazione più popolari per l’analisi dei dati e la statistica negli ultimi anni. Ecco alcuni dati sull’adozione di R:\n\nNumero di utenti: Secondo un sondaggio del 2022 condotto da Stack Overflow R è uno dei linguaggi di programmazione più popolare al mondo, utilizzato dal 4.7% degli sviluppatori. Nella lista è il primo software ad essere specificatamente pensato per applicazioni statistiche.\nNumero di pacchetti: Il Comprehensive R Archive Network (CRAN), il principale repository di pacchetti per R, contiene oltre 19.000 pacchetti. Questi pacchetti coprono una vasta gamma di applicazioni che vanno dalla statistica al machine learning, dalla visualizzazione e manipolazione dei dati alla stesura automatica di report, e altri task generici non necessariamente legati alle applicazioni di carattere statistico.\nUso aziendale: L’adozione di R nell’ambiente aziendale è aumentata negli ultimi anni, con molte aziende che lo utilizzano per l’analisi dei dati e la statistica.\n\nIn generale, R è diventato un linguaggio di programmazione popolare tra i data scientist, gli statistici e gli analisti dei dati grazie alla sua vasta gamma di pacchetti, alla sua flessibilità e alla sua potenza.\nQuore.",
    "crumbs": [
      "Syllabus",
      "R",
      "Introduzione a `R`"
    ]
  },
  {
    "objectID": "IntroduzioneR.html#installazione-di-r-e-rstudio",
    "href": "IntroduzioneR.html#installazione-di-r-e-rstudio",
    "title": "R: Introduzione",
    "section": "Installazione di R e Rstudio",
    "text": "Installazione di R e Rstudio\nR può essere scaricato gratuitamente dal sito ufficiale (www.r-project.org/) scegliendo la versione adatta al sistema operativo utilizzato.\nPer installare RStudio sul tuo computer bisogna visitare https://posit.co/download/rstudio-desktop/, scaricare la versione di RStudio Desktop compatibile con il sistema operativo e seguire le istruzioni per completare l’installazione.\n\n\n\n\n\n\nImportant\n\n\n\nUna differenza chiave da comprendere capire è quella tra R, il linguaggio di programmazione mentre RStudio è un’interfaccia ad R che consente di lavorare maggiore facilità con R.",
    "crumbs": [
      "Syllabus",
      "R",
      "Introduzione a `R`"
    ]
  },
  {
    "objectID": "IntroduzioneR.html#calcoli-di-base",
    "href": "IntroduzioneR.html#calcoli-di-base",
    "title": "R: Introduzione",
    "section": "Calcoli di Base",
    "text": "Calcoli di Base\nR come una semplice calcolatrice.\n\nAddizione, Sottrazione, Moltiplicazione e Divisione\n\n\n\nMatematica\nCodice R\nRisultato\n\n\n\n\n\\(3 + 2\\)\n3 + 2\n5\n\n\n\\(3 - 2\\)\n3 - 2\n1\n\n\n\\(3 \\cdot2\\)\n3 * 2\n6\n\n\n\\(3 / 2\\)\n3 / 2\n1.5\n\n\n\n\n\nEsponenti\n\n\n\nMatematica\nCodice R\nRisultato\n\n\n\n\n\\(3^2\\)\n3 ^ 2\n9\n\n\n\\(2^{(-3)}\\)\n2 ^ (-3)\n0.125\n\n\n\\(100^{1/2}\\)\n100 ^ (1 / 2)\n10\n\n\n\\(\\sqrt{100}\\)\nsqrt(100)\n10\n\n\n\n\n\nCostanti Matematiche\n\n\n\nMatematica\nCodice R\nRisultato\n\n\n\n\n\\(\\pi\\)\npi\n3.1415927\n\n\n\\(e\\)\nexp(1)\n2.7182818\n\n\n\n\n\nLogaritmi\nNota che useremo \\(\\ln\\) e \\(\\log\\) in modo interscambiabile per indicare il logaritmo naturale. Non c’è ln() in R, invece usa log() per indicare il logaritmo naturale.\n\n\n\nMatematica\nCodice R\nRisultato\n\n\n\n\n\\(\\log(e)\\)\nlog(exp(1))\n1\n\n\n\\(\\log_{10}(1000)\\)\nlog10(1000)\n3\n\n\n\\(\\log_{2}(8)\\)\nlog2(8)\n3\n\n\n\\(\\log_{4}(16)\\)\nlog(16, base = 4)\n2\n\n\n\n\n\nTrigonometria\n\n\n\nMatematica\nCodice R\nRisultato\n\n\n\n\n\\(\\sin(\\pi / 2)\\)\nsin(pi / 2)\n1\n\n\n\\(\\cos(0)\\)\ncos(0)\n1",
    "crumbs": [
      "Syllabus",
      "R",
      "Introduzione a `R`"
    ]
  },
  {
    "objectID": "IntroduzioneR.html#i-pacchetti-di-r",
    "href": "IntroduzioneR.html#i-pacchetti-di-r",
    "title": "R: Introduzione",
    "section": "I pacchetti di R",
    "text": "I pacchetti di R\nI pacchetti in R sono collezioni di funzioni, dati e codice compilato che sono sviluppati da terzi per aumentare le funzionalità di R. Sono fondamentali poiché permettono agli utenti di eseguire una vasta gamma di analisi e processi senza dover scrivere tanto codice. Ci sono migliaia di pacchetti disponibili su CRAN (Comprehensive R Archive Network), oltre a quelli disponibili su altre fonti come GitHub.\nPer installare un pacchetto in R, si utilizza la funzione install.packages(). Questa funzione scarica e installa il pacchetto desiderato dal repository CRAN. Una volta installato un pacchetto, è necessario caricarlo nella sessione di lavoro corrente usando la funzione library() per poter utilizzare le sue funzionalità.\nEcco un esempio di come installare e caricare il pacchetto ggplot2:\n\n# Installa il pacchetto ggplot2\ninstall.packages(\"ggplot2\")\n\n# Carica il pacchetto ggplot2\nlibrary(ggplot2)\n\nIn questo esempio, il primo comando installa ggplot2 e il secondo comando lo carica nella sessione di R attuale, rendendo disponibili tutte le sue funzioni e capacità.\nPer aggiornare un pacchetto, si può usare la funzione update.packages() e per rimuoverne uno, si usa remove.packages().\nI pacchetti possono anche essere installati da Rstudio, dal menu Strumenti|Installa pacchetti. Similarmente, i pacchetti possono essere aggiornati usando la voce Aggiorna pacchetti.\nAlcuni pacchetti di R includono dataset. Questi dataset sono particolarmente utili per apprendere tecniche di analisi dei dati, testare algoritmi, e per scopi didattici. Per esempio, wooldridge e Ecdat contengono vari dataset di carattere economico.\nPer accedere ai dataset inclusi nei pacchetti bisogna installarli e poi caricarli.\n\n# Installa il pacchetto Ecdat\ninstall.packages(\"Ecdat\")\n# Installa il pacchetto wooldridge\ninstall.packages(\"wooldridge\")\n\n# Carichiamo i pacchetti\nlibrary(Ecdat)\nlibrary(wooldridge)\n\n# Carica un dataset specifico, ad esempio 'Accident'\ndata(Accident)\n\nDopo aver installato e caricato Ecdat il comando data() consente di caricare uso specifico dataset in memoria per essere poi utilizzato nella nostra sessione.\nI dataset inclusi nei pacchetti offrono diversi vantaggi:\n\nFacilità di Accesso: Gli utenti possono accedere rapidamente a un’ampia gamma di dati senza doverli cercare o scaricare da fonti esterne.\nValidità e Affidabilità: I dati forniti nei pacchetti R sono generalmente ben documentati e validati, il che li rende affidabili per analisi e apprendimento.\nScopi Didattici: Sono ottimi per l’apprendimento e la pratica delle tecniche di analisi dei dati, offrendo casi di studio reali o simulati.\n\nIn conclusione, i pacchetti in R arricchiscono notevolmente l’esperienza dell’utente non solo con strumenti analitici avanzati ma anche con dati pronti per l’analisi, rendendo R un ambiente ancora più potente per l’analisi dei dati.\n\ninstall.packages(\"Ecdat\")\n\nAlternativamente, i pacchetti possono essere installati direttamente da RStudio usando il menu Tools (Strumenti).",
    "crumbs": [
      "Syllabus",
      "R",
      "Introduzione a `R`"
    ]
  },
  {
    "objectID": "IntroduzioneR.html#caricamento-dei-pacchetti",
    "href": "IntroduzioneR.html#caricamento-dei-pacchetti",
    "title": "R: Introduzione",
    "section": "Caricamento dei pacchetti",
    "text": "Caricamento dei pacchetti\nL’installazione rende disponibili i pacchetti sul proprio computer. Perché R possa utilizzare le funzionalità del pacchetto è necessario caricare i pacchetti utilizzando il comando library(). Ad esempio, per caricare il pacchetto Ecdat bisogna eseguire il seguente comando nella console R:\n\nlibrary(Ecdat)\n\n\nI dataset di Ecdat\nPer avere un’idea dei dataset messi a disposizione da Ecdat possiamo utilizzare la funzione di help\n\nhelp(package=\"Ecdat\")\n\nI dataset possono essere caricati nella sessione di R mediante il comando data(). Per esempio, il seguente comando\n\ndata(Caschool)\n\nrende disponibile il dataset Caschool. Il data set può essere richiamato semplicemente digitando il suo nome\n\nsummary(Caschool)\n\n    distcod              county                         district     grspan   \n Min.   :61382   Sonoma     : 29   Lakeside Union Elementary:  3   KK-06: 61  \n 1st Qu.:64308   Kern       : 27   Mountain View Elementary :  3   KK-08:359  \n Median :67760   Los Angeles: 27   Jefferson Elementary     :  2              \n Mean   :67473   Tulare     : 24   Liberty Elementary       :  2              \n 3rd Qu.:70419   San Diego  : 21   Ocean View Elementary    :  2              \n Max.   :75440   Santa Clara: 20   Pacific Union Elementary :  2              \n                 (Other)    :272   (Other)                  :406              \n    enrltot           teachers          calwpct          mealpct      \n Min.   :   81.0   Min.   :   4.85   Min.   : 0.000   Min.   :  0.00  \n 1st Qu.:  379.0   1st Qu.:  19.66   1st Qu.: 4.395   1st Qu.: 23.28  \n Median :  950.5   Median :  48.56   Median :10.520   Median : 41.75  \n Mean   : 2628.8   Mean   : 129.07   Mean   :13.246   Mean   : 44.71  \n 3rd Qu.: 3008.0   3rd Qu.: 146.35   3rd Qu.:18.981   3rd Qu.: 66.86  \n Max.   :27176.0   Max.   :1429.00   Max.   :78.994   Max.   :100.00  \n                                                                      \n    computer         testscr         compstu           expnstu    \n Min.   :   0.0   Min.   :605.5   Min.   :0.00000   Min.   :3926  \n 1st Qu.:  46.0   1st Qu.:640.0   1st Qu.:0.09377   1st Qu.:4906  \n Median : 117.5   Median :654.5   Median :0.12546   Median :5215  \n Mean   : 303.4   Mean   :654.2   Mean   :0.13593   Mean   :5312  \n 3rd Qu.: 375.2   3rd Qu.:666.7   3rd Qu.:0.16447   3rd Qu.:5601  \n Max.   :3324.0   Max.   :706.8   Max.   :0.42083   Max.   :7712  \n                                                                  \n      str            avginc           elpct           readscr     \n Min.   :14.00   Min.   : 5.335   Min.   : 0.000   Min.   :604.5  \n 1st Qu.:18.58   1st Qu.:10.639   1st Qu.: 1.941   1st Qu.:640.4  \n Median :19.72   Median :13.728   Median : 8.778   Median :655.8  \n Mean   :19.64   Mean   :15.317   Mean   :15.768   Mean   :655.0  \n 3rd Qu.:20.87   3rd Qu.:17.629   3rd Qu.:22.970   3rd Qu.:668.7  \n Max.   :25.80   Max.   :55.328   Max.   :85.540   Max.   :704.0  \n                                                                  \n    mathscr     \n Min.   :605.4  \n 1st Qu.:639.4  \n Median :652.5  \n Mean   :653.3  \n 3rd Qu.:665.9  \n Max.   :709.5",
    "crumbs": [
      "Syllabus",
      "R",
      "Introduzione a `R`"
    ]
  },
  {
    "objectID": "IntroduzioneR.html#il-sistema-di-help",
    "href": "IntroduzioneR.html#il-sistema-di-help",
    "title": "R: Introduzione",
    "section": "Il sistema di help",
    "text": "Il sistema di help\nIl sistema di help di R è un’importante risorsa per gli utenti che utilizzano questo software. Consente di accedere a informazioni dettagliate su tutte le funzioni e i pacchetti di R, nonché di comprendere meglio il funzionamento del linguaggio di programmazione R.\nOgni funzione in R ha la sua pagina di help alla quale si può accedere tramite la funzione help() o tramite il simbolo ? seguito dal nome della funzione della quale si vogliono ottenee inforrmazioni. Per esempio\n\nhelp(mean)\n\napre la pagina di help relativa alla funzione mean.\nLa pagina di help fornisce una descrizione della funzione, i suoi argomenti e tipologia dei valori che la funzione restituisce. La maggior parte delle funzioni in R hanno esempi di utilizzo nella loro pagina di help.\nPer ottenere aiuto sulle funzionalità di un pacchetto bisogna passare l’argomento packages seguito dal nome del pacchetto. Per esempio, il seguente comando mostra la pagina di help per il pacchetto ggplot2\n\nhelp(packages=\"ggplot2\")",
    "crumbs": [
      "Syllabus",
      "R",
      "Introduzione a `R`"
    ]
  },
  {
    "objectID": "IntroduzioneR.html#primi-grafici-con-ggplot2",
    "href": "IntroduzioneR.html#primi-grafici-con-ggplot2",
    "title": "R: Introduzione",
    "section": "Primi grafici con ggplot2",
    "text": "Primi grafici con ggplot2\nI passi per ottenere un grafico con ggplot2 sono:\n\nInstallare e caricare il pacchetto ggplot2 utilizzando il comando install.packages(\"ggplot2\") seguito da library(ggplot2).\nCaricare i dati che si desidera visualizzare in un data frame.\nCreare un oggetto ggplot utilizzando la funzione ggplot(). Gli argomenti di questa funzione sono il data.frame e la funzione aes mediante la quale possono essere specificate le variabili che si desidera utilizzare per l’asse x e l’asse y.\nAggiungere i livelli e gli stili al grafico utilizzando una serie di geometrie come geom_point(), geom_line(), geom_bar(). Queste funzioni definiscono le modalità di rappresentazione dei dati nel grafico (ad esempio, con punti, linee, barre, ecc.).\nAggiungere etichette, titoli e altre personalizzazioni utilizzando funzioni come labs(), xlab(), ylab(), ggtitle() e altre funzioni di personalizzazione.\n\nEcco un esempio di codice di base per creare un grafico a dispersione utilizzando ggplot2:\n\nlibrary(ggplot2)\n# Caricare i dati\ndata(Caschool)\n# Creare l'oggetto ggplot\nscatterplot &lt;- ggplot(Caschool, aes(x = str, y = testscr))\n# Aggiungere i punti al grafico\nscatterplot &lt;- scatterplot + geom_point(col = \"darkgreen\")\n# Aggiungere etichette e titoli\nscatterplot &lt;- scatterplot +\n  labs(title = \"Grafico a dispersione - testscr/str\",\n      x = \"Rapporto studenti insegnanti\", y = \"Punteggi nei test\")\n# Visualizzare il grafico\nscatterplot\n\n\n\n\n\n\n\n\nQuesto è solo un esempio molto semplice, ma ggplot2 offre molte altre funzionalità per la creazione di grafici avanzati e personalizzati. Per esempio, possiamo cambiare il tema grafico usando theme_bw() che elimina i colori\n\nscatterplot &lt;- scatterplot + theme_bw()\nscatterplot\n\n\n\n\n\n\n\n\nPossiamo anche costruire grafici raggruppati per variabile. Per ottenere un grafico a dispersione per str e testscr per le due tipologie di scuole grspan==\"KK-06\" egrspan=“KK-08”`:\n\nscatterplot &lt;- scatterplot + facet_wrap(~grspan)\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLa funzione facet_wrap() in ggplot2 è utilizzata per dividere i dati in sottoinsiemi basati su una variabile e creare una griglia di grafici separati, ognuno dei quali visualizza i dati per uno dei livelli della variabile.\nLa sintassi di base di facet_wrap() è la seguente:\n\nfacet_wrap(~variable, nrow = x, ncol = y)\n\nDove variable è la variabile che si vuole utilizzare per suddividere i dati in sottoinsiemi (nell precedente esempio abbiamo utilizzato grspan) e nrow e ncol specificano il numero di righe e colonne della griglia di grafici che si desidera creare.\n\n\nPossiamo anche aggiungere le rette di regression a ciascun grafico usando la geometria geom_smooth\n\nscatterplot + geom_smooth(method=\"lm\", col = \"darkred\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Syllabus",
      "R",
      "Introduzione a `R`"
    ]
  },
  {
    "objectID": "IntroduzioneR.html#prime-manipolazione-di-dati-usando-dplyr",
    "href": "IntroduzioneR.html#prime-manipolazione-di-dati-usando-dplyr",
    "title": "R: Introduzione",
    "section": "Prime manipolazione di dati usando dplyr",
    "text": "Prime manipolazione di dati usando dplyr\n\nlibrary(dplyr)\n\nCalcoliamo la media e la standard deviation di testscr\n\nCaschool |&gt;\n  summarize(m = mean(testscr), s = sd(testscr))\n\n         m        s\n1 654.1565 19.05335\n\n\nPossiamo anche calcolare la media, la deviazione standard e il numero di osservazioni per il gruppo di scuole che hanno un rapporto studenti insegnanti minore di 20, \\(str&lt;20\\) e il gruppo di scuole che hanno un rapporto maggiore o uguale a venti (\\(str&gt;20\\)) usando la funzione group_by seguita dall’indicazione del gruppo (nel nostro caso str&lt;20):\n\ndf_1 &lt;- Caschool |&gt;\n  group_by(str&lt;20) |&gt;\n  summarize(m = mean(testscr),\n            s = sd(testscr),\n            n = n())\n\nIl risultato delle manipolazioni è un tibble — un sorta di data.frame più flessibile definito nel pacchetto dplyr.\ndf_1 può essere a sua volta utilizzato per altri calcoli e si comporta in tutto e per tutto come un normale data.frame.\n\ndf_1\n\n# A tibble: 2 × 4\n  `str &lt; 20`     m     s     n\n  &lt;lgl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 FALSE       650.  17.9   182\n2 TRUE        657.  19.4   238\n\n\n\n\n\n\n\n\nNote\n\n\n\nLe maggiori differenze fra i tibble e i data.frame sono:\n\nTipo di output: Il metodo di visualizzazione di un tibble è più compatto e leggibile rispetto a quello di un data.frame.\nComportamento dei nomi delle variabili: in un tibble, i nomi delle variabili sono sempre conservati e non vengono mai modificati (ad esempio, i nomi delle variabili non vengono convertiti in stringhe). In un data.frame, i nomi delle variabili possono essere modificati o convertiti in stringhe.\nComportamento di default dei dati mancanti: in un tibble, i dati mancanti vengono visualizzati in modo più chiaro rispetto a un data.frame.\nComportamento della subsetting: in un tibble, il subsetting (o l’estrazione di sottoinsiemi di dati) è più rigoroso rispetto a un data.frame, in quanto conserva sempre la classe del tibble, anche se viene restituito un singolo valore.\nFunzioni dplyr: un tibble è progettato per essere compatibile con le funzioni del pacchetto dplyr, che sono utilizzate per manipolare i dati.\n\nI tibble sono una versione leggermente migliorata dei tradizionali data.frame, progettata per semplificare l’analisi dei dati in R. Tuttavia, i data.frame sono ancora molto utilizzati e sono l’oggetto dati di base per molti pacchetti in R. Fortunatamente, molti dei comandi di R disegnati per i data.frame funzionano anche per i tibble.\n\n\nAbbiamo visto che una stima della differenza dei valori attesi fra i due gruppi \\[\n\\Delta = E(testscr|str&lt;20) - E(testscr|str\\geqslant20)\n\\] può essere ottenuta calcolando la differenza delle medie campionarie dei due gruppi e può essere ottenuta usando le informazioni in df_1:\n\n## Differenza delle medie campionarie\ndf_1$m[2]-df_1$m[1]\n\n[1] 7.37241\n\n\ne, quindi, \\[\n\\hat{\\Delta} = \\overline{testscr}_{str&lt;20} - \\overline{testscr}_{str\\geqslant20} = 7.3724101.\n\\]\nCon le stesse informazioni è possibile calcolare l’intervallo di confidenza al 95% per \\(\\Delta\\) che è dato da \\[\n\\hat{\\Delta}\\pm1.96\\times\\sqrt{\\frac{s_{str&lt;20}^{2}}{n_{str&lt;20}}+\\frac{s_{str\\geqslant20}^{2}}{n_{str\\geqslant20}}}.\n\\] Usando le informazioni contenute in df_1 otteniamo: \\[\n(3.7979806, 10.9468397)\n\\]",
    "crumbs": [
      "Syllabus",
      "R",
      "Introduzione a `R`"
    ]
  },
  {
    "objectID": "IntroduzioneR.html#qualcosa-di-più-complicato",
    "href": "IntroduzioneR.html#qualcosa-di-più-complicato",
    "title": "R: Introduzione",
    "section": "Qualcosa di più complicato",
    "text": "Qualcosa di più complicato\nInvece di stimare la variazione del valore atteso rispetto alle due macro-categorie (\\(str&lt;20\\) e \\(str\\geqslant20\\)) possiamo provare a stimare il valore atteso condizionatamente a piccoli intervalli. Per esempio, possiamo voler stimare la seguente differenza \\[\nE(testscr|20.1 &lt; str \\leqslant 20.6) - E(testscr|20.6 &lt; str \\leqslant 21.1)\n\\] oppure \\[\nE(testscr|21.6 &lt; str \\leqslant 22) - E(testscr|22 &lt; str \\leqslant 22.5).\n\\]\nPer stimare queste quantità utilizzando i dati nel campion, è necessario calcolare la media campionaria per le scuole che hanno un \\(str\\) che appartiene a ciascun intervallo. Per costruire questi intervalli, possiamo utilizzare la funzione cut(). La funzione cut() è utilizzata per suddividere un vettore numerico in più intervalli.\n\n\n\n\n\n\nNote\n\n\n\nLa sintassi della funzione cut() è la seguente:\n\ncut(x, breaks,\n       labels = NULL,\n       include.lowest = FALSE,\n       right = TRUE,\n       dig.lab = 3, ...)\n\ndove:\n\nx è il vettore numerico da suddividere in bin\nbreaks è un vettore contenente i valori di taglio, ovvero i punti di separazione tra i bin. Questo può essere specificato in diversi modi, ad esempio tramite un numero intero che specifica il numero di bin desiderati o un vettore numerico che specifica i limiti di ogni bin.\nlabels è un vettore di etichette per ogni bin. Se non specificato, gli intervalli saranno etichettati in base ai loro limiti.\ninclude.lowest indica se includere o meno il valore più basso del vettore x nel primo bin. Il valore predefinito è FALSE.\nright indica se i bin devono essere chiusi a destra o a sinistra. Il valore predefinito è TRUE, ovvero i bin sono chiusi a destra.\ndig.lab è il numero di cifre decimali da utilizzare per le etichette, se specificato.\n... sono altri argomenti opzionali.\n\n\n\nEcco un esempio di utilizzo della funzione cut() per suddividere str in tre bin di uguale ampiezza:\n\nstr_cat3 &lt;- cut(Caschool$str, 3)\nsummary(str_cat3)\n\n  (14,17.9] (17.9,21.9] (21.9,25.8] \n         73         305          42 \n\n\nIn questo caso il comando summary produce una tavola di frequenza con il numero delle scuole in ciascuna delle tre categorie (14,17.9], (17.9,21.9], (21.9,25.8].\nPossiamo anche specificare i breaks in modo esplicito\n\nstr_cat3 &lt;- cut(Caschool$str, c(13,20, 21, 26))\nsummary(str_cat3)\n\n(13,20] (20,21] (21,26] \n    243      86      91 \n\n\nIn questo ultimo caso, gli intervalli sono (13,20], (20,21], (21,26].\n\n\n\n\n\n\nNote\n\n\n\nstr_cat3 e, in generale, l’output di cut è un oggetto di classe factor. Questo tipo di variabili sono utilizzate per rappresentare variabili categoriche o qualitative, ovvero variabili che possono assumere un numero limitato di categorie o livelli (come il numero di intervalli nel nostro questo caso). La funzione summary() restituisce una tavola di frequenza per questo tipo di variabili perchè non avrebbe senso calcolare media, mediana e le altre quantità che sono solitamente restituita da summary quando l’argomento è una variabile di tipo numeric.\n\n\n\nlibrary(knitr)\n\ndf_2 &lt;- Caschool |&gt;\n1  mutate(str_cat = cut(str, 25)) |&gt;\n2  group_by(str_cat) |&gt;\n3  summarize(m = mean(testscr), s = sd(testscr), n = n())\n\n4df_2 |&gt; kable()\n\n\n1\n\nla funzione cut per dividere str in 25 intervalli;\n\n2\n\ngroup_by suddivide Caschool in gruppi specificati da str_cat\n\n3\n\nsummarize calcola la media, la standard deviation e il numero di osservazioni per ciascun gruppo\n\n4\n\nkabble restituisce la tavole dei valori\n\n\n\n\n\n\n\nstr_cat\nm\ns\nn\n\n\n\n\n(14,14.5]\n646.0500\n14.778549\n2\n\n\n(14.5,14.9]\n681.0750\n20.117197\n2\n\n\n(14.9,15.4]\n669.0583\n27.490814\n6\n\n\n(15.4,15.9]\n666.4300\n16.397313\n5\n\n\n(15.9,16.4]\n657.4625\n31.451677\n4\n\n\n(16.4,16.8]\n656.4000\n22.941905\n12\n\n\n(16.8,17.3]\n655.1200\n19.810342\n10\n\n\n(17.3,17.8]\n661.8417\n21.060798\n24\n\n\n(17.8,18.2]\n660.9820\n21.377905\n25\n\n\n(18.2,18.7]\n655.2611\n19.005516\n27\n\n\n(18.7,19.2]\n659.1279\n17.430926\n43\n\n\n(19.2,19.7]\n654.1674\n18.195814\n46\n\n\n(19.7,20.1]\n652.2074\n16.740564\n54\n\n\n(20.1,20.6]\n652.5538\n18.840601\n39\n\n\n(20.6,21.1]\n650.9581\n18.103751\n37\n\n\n(21.1,21.6]\n645.9219\n17.020841\n32\n\n\n(21.6,22]\n651.4000\n19.099852\n15\n\n\n(22,22.5]\n650.9208\n14.543992\n12\n\n\n(22.5,23]\n639.0750\n16.285659\n10\n\n\n(23,23.4]\n643.8833\n17.057669\n6\n\n\n(23.4,23.9]\n658.9750\n7.672091\n2\n\n\n(23.9,24.4]\n676.8500\nNA\n1\n\n\n(24.4,24.9]\n651.2000\nNA\n1\n\n\n(24.9,25.3]\n642.3833\n23.492216\n3\n\n\n(25.3,25.8]\n659.5750\n7.601398\n2\n\n\n\n\n\nAlcuni degli errori standard sono uguali a NA perché in almeno uno dei gruppi il numero di osservazioni è inferiore a 2, il numero minimo per poter calcolare questa misura dispersione.\nManipolando df_2, possiamo calcolare le differenze nelle media campionarie fra due intervalli adiacenti.\n\n1df_3 &lt;- df_2 |&gt; mutate(Delta_str = paste0(str_cat, \"-\", lag(str_cat)),\n2               Delta_testscr = m - lag(m),\n3               stderr = sqrt(s^2/n + lag(s^2/n))) |&gt;\n4               select(Delta_str, Delta_testscr, stderr)\n\n5kable(df_3)\n\n\n1\n\nla variabile Delta_str è uguale alla differenza dei due intervalli ottenuta incollando paste0 l’intervallo di ciascuna riga con quella precedente (la funzione lag(m) associa il valore di m della riga precedente). Questa variabile è utile per annotare l’asse delle ascisse del grafico che produrremo prodotto in seguito.\n\n2\n\nDelta_str contiene la differenze delle medie campionari di due intervalli successivi. Queste differenze sono calcolate usando m - lag(m) e quindi Delta_testscr è uguale a m, la media per l’intervallo, meno il valore della media per l’intervallo che preceda la riga in considerazione (lag(m))\n\n3\n\nstderr è l’errore standard della differenza delle medie in ciascun intervallo che è uguale a\n\n4\n\nselezioniamo le variabili che ci servono per il grafico mediante la funzione select()\n\n5\n\nkable(df_3) formatta il data.frame come una tavolo html. La funzione kable è contenuta nel pacchetto knitr.\n\n\n\n\n\n\n\nDelta_str\nDelta_testscr\nstderr\n\n\n\n\n(14,14.5]-NA\nNA\nNA\n\n\n(14.5,14.9]-(14,14.5]\n35.0249939\n17.650880\n\n\n(14.9,15.4]-(14.5,14.9]\n-12.0166524\n18.119279\n\n\n(15.4,15.9]-(14.9,15.4]\n-2.6283122\n13.406411\n\n\n(15.9,16.4]-(15.4,15.9]\n-8.9675232\n17.351552\n\n\n(16.4,16.8]-(15.9,16.4]\n-1.0624949\n17.063496\n\n\n(16.8,17.3]-(16.4,16.8]\n-1.2799978\n9.116243\n\n\n(17.3,17.8]-(16.8,17.3]\n6.7216634\n7.597797\n\n\n(17.8,18.2]-(17.3,17.8]\n-0.8596676\n6.063179\n\n\n(18.2,18.7]-(17.8,18.2]\n-5.7208842\n5.626609\n\n\n(18.7,19.2]-(18.2,18.7]\n3.8667941\n4.521517\n\n\n(19.2,19.7]-(18.7,19.2]\n-4.9605207\n3.776710\n\n\n(19.7,20.1]-(19.2,19.7]\n-1.9599775\n3.519561\n\n\n(20.1,20.6]-(19.7,20.1]\n0.3464399\n3.780410\n\n\n(20.6,21.1]-(20.1,20.6]\n-1.5957303\n4.237894\n\n\n(21.1,21.6]-(20.6,21.1]\n-5.0362433\n4.232187\n\n\n(21.6,22]-(21.1,21.6]\n5.4781291\n5.776997\n\n\n(22,22.5]-(21.6,22]\n-0.4791768\n6.476697\n\n\n(22.5,23]-(22,22.5]\n-11.8458333\n6.644515\n\n\n(23,23.4]-(22.5,23]\n4.8083476\n8.661194\n\n\n(23.4,23.9]-(23,23.4]\n15.0916951\n8.827486\n\n\n(23.9,24.4]-(23.4,23.9]\n17.8749390\nNA\n\n\n(24.4,24.9]-(23.9,24.4]\n-25.6499634\nNA\n\n\n(24.9,25.3]-(24.4,24.9]\n-8.8166707\nNA\n\n\n(25.3,25.8]-(24.9,25.3]\n17.1916097\n14.589449\n\n\n\n\n\nSi noti che per prima riga la differenza delle medie è NA. Il motivo è che non c’è un intervallo con valori più piccoli di per poter calcolare la differenza. La seconda riga ci dice che \\[\n\\overline{testscr}_{str \\in (14.5,14.9]} - \\overline{testscr}_{str \\in (14,14.5]}  = 35.0249939,\n\\] e che quindi scuole con classi con \\((14.5,14.9]\\) studenti per insegnante hanno punteggi più alti di circa 35.0249939 punti rispetto a quelle con \\(str\\in (14,14.5]\\). Questo valore positivo (classi più piccole hanno test score più bassi) e molto grande (quasi due volte la deviazione standard dei punteggi in tutto il campione) è dovuto al fatto che stiamo stimando la differenza dei valori attesi usando soltanto 4 scuole. Un numero troppo esigue per aspettarci che la stima sia in qualche modo “vicina” a quella che potremmo stimare se avessimo a disposizione i dati nel campione.\nLa terza riga ci dice che \\[\n\\overline{testscr}_{str \\in (14.9,15.4]} - \\overline{testscr}_{str \\in (14.5,14.9]}  = -12.0166524,\n\\] e che quindi scuole con \\(str\\in (14.9,15.4]\\) hanno punteggi più alti di circa -12.0166524 punti rispetto a quelle con \\(str\\in (14.5,14.9]\\). E così via per le altre righe.\nE’ molto probabile che tutte le stime e non soltanto quelle della prima riga siano particolarmente imprecise visto che sono tutte basate su un numero esiguo di osservazioni. Per quantificare la loro precisione, o la loro imprecisione, possiamo costruire l’intervallo di confidenza (al 95%) per ciascun valore dell’intervallo di \\(str\\). L’intervallo di confidenza al 95% è:\n\nc(df_3$Delta_testscr - 1.96 * df_3$stderr,\n  df_3$Delta_testscr + 1.96 * df_3$stderr)\n\ndove stderr è l’errore standard della differenza delle media che è stato calcolato nel precdedente blocco di codice.\nPer aggiungere l’intervallo di confidenza a df_3 possiamo usare dplyr e la funzione mutate:\n\ndf_3 &lt;- df_3 |&gt;\n1  mutate(ci_sx = Delta_testscr - 1.96 * stderr,\n2         ci_dx = Delta_testscr + 1.96 * stderr)\n\nkable(df_3)\n\n\n1\n\nl’estremo sinistro dell’intervallo di confidenza;\n\n2\n\nl’estremo destro dell’intervallo di confidenza;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDelta_str\nDelta_testscr\nstderr\nci_sx\nci_dx\n\n\n\n\n(14,14.5]-NA\nNA\nNA\nNA\nNA\n\n\n(14.5,14.9]-(14,14.5]\n35.0249939\n17.650880\n0.4292697\n69.620718\n\n\n(14.9,15.4]-(14.5,14.9]\n-12.0166524\n18.119279\n-47.5304392\n23.497134\n\n\n(15.4,15.9]-(14.9,15.4]\n-2.6283122\n13.406411\n-28.9048769\n23.648253\n\n\n(15.9,16.4]-(15.4,15.9]\n-8.9675232\n17.351552\n-42.9765654\n25.041519\n\n\n(16.4,16.8]-(15.9,16.4]\n-1.0624949\n17.063496\n-34.5069476\n32.381958\n\n\n(16.8,17.3]-(16.4,16.8]\n-1.2799978\n9.116243\n-19.1478335\n16.587838\n\n\n(17.3,17.8]-(16.8,17.3]\n6.7216634\n7.597797\n-8.1700183\n21.613345\n\n\n(17.8,18.2]-(17.3,17.8]\n-0.8596676\n6.063179\n-12.7434989\n11.024164\n\n\n(18.2,18.7]-(17.8,18.2]\n-5.7208842\n5.626609\n-16.7490376\n5.307269\n\n\n(18.7,19.2]-(18.2,18.7]\n3.8667941\n4.521517\n-4.9953794\n12.728967\n\n\n(19.2,19.7]-(18.7,19.2]\n-4.9605207\n3.776710\n-12.3628722\n2.441831\n\n\n(19.7,20.1]-(19.2,19.7]\n-1.9599775\n3.519561\n-8.8583162\n4.938361\n\n\n(20.1,20.6]-(19.7,20.1]\n0.3464399\n3.780410\n-7.0631635\n7.756043\n\n\n(20.6,21.1]-(20.1,20.6]\n-1.5957303\n4.237894\n-9.9020023\n6.710542\n\n\n(21.1,21.6]-(20.6,21.1]\n-5.0362433\n4.232187\n-13.3313288\n3.258842\n\n\n(21.6,22]-(21.1,21.6]\n5.4781291\n5.776997\n-5.8447856\n16.801044\n\n\n(22,22.5]-(21.6,22]\n-0.4791768\n6.476697\n-13.1735022\n12.215148\n\n\n(22.5,23]-(22,22.5]\n-11.8458333\n6.644515\n-24.8690823\n1.177416\n\n\n(23,23.4]-(22.5,23]\n4.8083476\n8.661194\n-12.1675926\n21.784288\n\n\n(23.4,23.9]-(23,23.4]\n15.0916951\n8.827486\n-2.2101770\n32.393567\n\n\n(23.9,24.4]-(23.4,23.9]\n17.8749390\nNA\nNA\nNA\n\n\n(24.4,24.9]-(23.9,24.4]\n-25.6499634\nNA\nNA\nNA\n\n\n(24.9,25.3]-(24.4,24.9]\n-8.8166707\nNA\nNA\nNA\n\n\n(25.3,25.8]-(24.9,25.3]\n17.1916097\n14.589449\n-11.4037108\n45.786930\n\n\n\n\n\nCome si vede chiaramente dall’analisi della tabella, tutti gli intervalli di confidenza sono molto ampi. Come preannunciato, i dati a nostra disposizione non sono abbastanza informativi per poter stimare tutte le differenze su intervalli così poco numerosi. Anche nel caso in cui considerassimo la differenza delle medie in scuole con \\(str\\in(18.7,19.2]\\) (27 scuole) e scuole con \\(str\\in (18.2,18.7]\\) (47 scuole), l’intervallo di confidenza è molto ampio: \\((-4.9953794, 12.7289675)\\). Un intervallo di confidenza così ampio implica che non possiamo neanche quantificare con l’appropriata confidenza il segno della differenza che potrebbe essere -12 o 2.\nLa rappresentazione grafica delle informazioni contenute in una tavole facilita spesso la comprensione dei risultati.\n\n1ggplot(df_3, aes(x=Delta_str,y=Delta_testscr)) +\n2  geom_pointrange(aes(ymin=ci_sx, ymax=ci_dx)) +\n3  geom_hline(yintercept = 0, col = \"darkred\") +\n  theme_bw() +\n  xlab(\"Intervalli str\") + ylab(\"Delta Media testscr\") +\n4  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n1\n\nsull’ascissa abbiamo la differenza degli intervalli, sull’ordinata la differenza delle medie;\n\n2\n\nla geometria utilizzata è geom_pointrange, che “plotta” la differenza delle medie (il punto) e l’intervallo di confidenza al 95% definito da ((ci_sx, ci_dx)) e rappresentato dalle linee che si estendono verticalmente.\n\n3\n\ngeom_hline(yintercept=0, col = \"darkred\") produce una riga orizzonate di colore rosso scuro\n\n4\n\nle etichette dell’asse della x sono ruotate di 90 gradi per favorire la loro leggibilità.\n\n\n\n\n\n\n\n\n\n\n\nIl grafico mostra che la stima di molte delle differenze è negativa e che gli intervalli di confidenza (eccettuati i tre casi in cui non è possibile costruire l’intervallo di confidenza) sono troppo ampi per poter concludere che il valore della differenza nella popolazione sia negativa",
    "crumbs": [
      "Syllabus",
      "R",
      "Introduzione a `R`"
    ]
  },
  {
    "objectID": "Dummy_ilfs.html",
    "href": "Dummy_ilfs.html",
    "title": "Variabili Dummy and Dati Italiani",
    "section": "",
    "text": "Le variabili dummy sono utilizzate per analizzare variabili categoriche.",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili dummy"
    ]
  },
  {
    "objectID": "Dummy_ilfs.html#variabili-dummy",
    "href": "Dummy_ilfs.html#variabili-dummy",
    "title": "Variabili Dummy and Dati Italiani",
    "section": "",
    "text": "Le variabili dummy sono utilizzate per analizzare variabili categoriche.",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili dummy"
    ]
  },
  {
    "objectID": "Dummy_ilfs.html#dati-sui-lavoratori-italiani",
    "href": "Dummy_ilfs.html#dati-sui-lavoratori-italiani",
    "title": "Variabili Dummy and Dati Italiani",
    "section": "Dati sui lavoratori italiani",
    "text": "Dati sui lavoratori italiani\nUtilizeremo un dataset contenente alcuni variabili che descrivono le caratteristiche di una campione di lavoratori dipendenti italiani.\n\n\n\n\n\n\n\nVariabile\nDescrizione\n\n\n\n\nretric\nRetribuzione mensile netta\n\n\ngenere\nGenere del lavoratore\n\n\nregione\nRegione di residenza\n\n\nedulev\nLivello di istruzione\n\n\nistruzione_anni\nAnni di istruzione completati\n\n\neta\nEtà del lavoratore\n\n\ntitolo10\nTitolo di studio\n\n\ndetind\nTipo di contratto di lavoro (indeterminato, determinato)\n\n\npiepar\nTipo di orario di lavoro (pieno, parziale)\n\n\ntenure\nAnni di lavoro presso l’attuale datore di lavoro\n\n\ncittadinanza\nSe il lavoratore ha la cittadinanza italiana\n\n\nsg13\nLuogo di nascita\n\n\norelav\nOre di lavoro settimanali\n\n\ngenere_dummy\nVariabile dummy per il genere (1 = Femmina, 0 = Maschio)\n\n\n\nIl file può essere scaricato qua.",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili dummy"
    ]
  },
  {
    "objectID": "Dummy_ilfs.html#salari-per-genere",
    "href": "Dummy_ilfs.html#salari-per-genere",
    "title": "Variabili Dummy and Dati Italiani",
    "section": "Salari per genere",
    "text": "Salari per genere\nConsideriamo la variabile genere:\n\nhead(df[, \"genere\"])\n\n# A tibble: 6 × 1\n  genere \n  &lt;chr&gt;  \n1 Maschio\n2 Femmina\n3 Femmina\n4 Maschio\n5 Maschio\n6 Femmina\n\n\nLa variabile assume il valore maschio o femmina a seconda che l’individua sia di genere maschile o femminile. Possiamo creare la variabile manualmente nel modo seguente\n\ndf &lt;- df %&gt;% mutate(Maschio = ifelse(genere==\"Maschio\", 1, 0))\n\nLa variabile Maschio prende il valore 1 se il genere dell’individuo è maschile e 0 altrimenti.\nPossiamo utilizzare questa variabile per analizzare la variabile genere. Per esempio, possiamo prendere la media di Maschio\n\nmean(df$Maschio)\n\n[1] 0.5345987\n\n\nLa media nel caso di una variabile dummy è la percentuale di osservazioni che prendono il valore uno. Nel caso dei nostri dati, \\(\\bar{X}_{maschi}=0.535\\) indica che circa il 53.46% dei lavoratori nel nostro campione è maschio.\nNon c’è nulla di speciale nella scelta di “maschio” come valore per assegnare alla variabile dummy il valore 1. Possiamo infatti creare la variabile dummy Femmina con il valore 1 qualora il lavoratore sia di genere femminile.\n\ndf &lt;- df %&gt;% mutate(Femmina = ifelse(genere==\"Femmina\", 1, 0))\n\nIn questo caso, \\(\\bar{X}_{femmina}=0.465\\) indica che circa il 46.54 dei lavoratori nel nostro campione è una donna (questa percentuale non è altro che \\(1-\\bar{X}_{maschio}\\).)\nCome possiamo usare questa variabile in una regressione? Possiamo usare la dummy come variabile indipendente.\n\nlm_maschi &lt;- feols(retric~Maschio, data = df)\nlm_maschi\n\nOLS estimation, Dep. Var.: retric\nObservations: 138,618 \nStandard-errors: IID \n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 1171.258    1.99513 587.058 &lt; 2.2e-16 ***\nMaschio      281.449    2.72871 103.144 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 506.7   Adj. R2: 0.071272\n\n\nL’interpretazione del coefficiente sulla dummy non è altro che la differenza dela salario fra uomini (Maschio=1) e donne (Maschio=0). Quindi, visto che \\(\\hat{\\beta}_1 = 281.4492935\\), gli uomini nel nostro campione guadagnano in media \\(281.4492935\\) euro piu’ delle donne. L’intercetta è in questa regressione la media (sempre nel campione) del salario delle donne (l’intercetta è interpretabile come la media della variabile dipendente quando i regressori sono uguali a zero).\nSe utilizziamo la dummy Femmina invece della variabile dummy Maschio, i coefficienti stimati saranno diversi, ma la loro interpetazione finale sarà identica.\n\nlm_femmine &lt;- feols(retric~Femmina, data = df)\nlm_femmine\n\nOLS estimation, Dep. Var.: retric\nObservations: 138,618 \nStandard-errors: IID \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 1452.708    1.86154  780.381 &lt; 2.2e-16 ***\nFemmina     -281.449    2.72871 -103.144 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 506.7   Adj. R2: 0.071272\n\n\nIn questa nuova regression, \\(\\hat{\\beta}_1 = 1452.7076446\\), e quindi possiamo dire che le donne nel nostro campione guadagnano in media \\(1452.7076446\\) euro in meno degli uomini. L’intercetta è invece la media del salario degli uomini.\nSe calcolassimo le medie e la differenze delle medie manualemnte senza usare la regressione otterremo gli stessi risultati. Per esempio, il salario medio dei lavoratori di sesso maschile è esattamente uguale al coefficiente \\(\\hat{\\beta}_0\\) nella regressione che usa la veriaile dummy Femmina:\n\nmean(df$retric[df$genere==\"Maschio\"])\n\n[1] 1452.708\n\n\nPer avere una completa panaromica della media di retric per genere possiamo usare la funzione group_by e summarize di dplyr”\n\ndf %&gt;% group_by(genere) %&gt;% summarize(mean(retric))\n\n# A tibble: 2 × 2\n  genere  `mean(retric)`\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Femmina          1171.\n2 Maschio          1453.\n\n\nI coefficienti delle due regressioni stimano i corrispondenti valori della popolazione: \\(\\hat{\\beta}_0\\) stima la media nella popolazione dei salari per il caso in cui la variabile dummy è zero; \\(\\hat{\\beta}_1\\) stima la differenza dei salari medi nella poplazione fra i lavoratori con variabile dummy uguale a 1 e variabile dummy uguale a zero. Quindi possiamo chiederci se questa differenza nei salari che vediamo nel campioni è probabile che permangano qualora avessimo a disposizione l’intera popolazione. L’intervallo di confidenza risponde a questa domanda fornendoci un intervallo di valori probabili della differenza salariale fra uomini e donne nel campione:\n\nconfint(lm_maschi)\n\n                2.5 %    97.5 %\n(Intercept) 1167.3479 1175.1688\nMaschio      276.1011  286.7975\n\n\nGli intervalli sono molto piccoli (come era prevedibile vista la grandezza del campione) e ci dice che questa differenza non è probabilmenmte dovuta al particolare compione che stiamo analizzando.\n\nSalari per regione\nUna variabile particolarmente interessante del nostro dataset è la variabile regione che fornisce la regione di residenza di ciascun lavoratore nel nostro campione\n\nhead(df[,\"regione\"])\n\n# A tibble: 6 × 1\n  regione  \n  &lt;chr&gt;    \n1 Puglia   \n2 Lombardia\n3 Piemonte \n4 Lombardia\n5 Umbria   \n6 Campania \n\n\nL’esistenza di differenza salariali nelle regioni italiane può essere investigata mediante un grafico che mostra la distribuzione di retric per ciascuna regione:\n\nggplot(df, aes(x = retric)) + \n  xlab(\"Regione\") + \n  ylab(\"Retribuzione netta\") +\n  geom_histogram(aes(y=stat(density)))  + \n  theme_minimal() + \n  theme(axis.text.x = \n          element_text(\n            angle = 25,\n            vjust = 0.5,\n            hjust = 1,\n            size = 6), \n        axis.text.y = \n          element_text(size = 6)\n        ) + facet_wrap(~regione)\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(df, aes(y = retric, x = fct_reorder(regione, retric))) + \n  xlab(\"Regione\") + \n  ylab(\"Retribuzione netta\") +\n  geom_boxplot(outlier.alpha = 0.6)  + \n  theme_minimal() + \n  theme(axis.text.x = element_text(\n  angle = 45,\n  vjust = 0.5,\n  hjust = 1\n))\n\n\n\n\n\n\n\n\nPur essendo una variabile categorica, regione prende piu’ di due valori e quindi non è possibile codificare l’informazione contenuta in essa mediante un’unica variabile dummy. In questo caso creiama una dummy per ciascuna regione: una dummy Lombardia che prende il valore 1 per quei lavoratori che risiedono in Lombardia e 0 per gli altri, una dummy Sicilia per i lavoratori che risiedono in Sicilia e 0 per gli altri, e così via.\nE’ possibile creare queste dummy manualmente:\n\ndf &lt;- df %&gt;% mutate(Lombardia = ifelse(regione==\"Lombardia\", 1, 0),\n                    Sicilia = ifelse(regione==\"Sicilia\", 1, 0),\n                    .\n                    .\n                    )\n\nLa creazione manuale è però tediosa e prona ad errori. È preferibile automatizzare il processo di creazione delle dummy mediante il pachetto fastDummies.\n\nlibrary(fastDummies)\ndf &lt;- fastDummies::dummy_cols(df, select_columns = \"regione\", \n                              omit_colname_prefix = TRUE)\n\nDopo l’esecuzione del codice, df conterrà una variabile dummy per ciascuna regione. Possiamo adesso calcolare la media di ciascuna dummy così creata per ottenere la percentuale dei residenti in ciascuna regione. Per esempio,\n\nmean(df$Calabria)\n\n[1] 0.01961506\n\n\nindica che circa il 1.96% dei lavoratori del nostro campione è residente in Calabria.\nQual è l’interpretazione della regressione in cui aggiungiamo tutte le dummy?\n\nlm_regione &lt;- feols(retric~Abruzzo+Basilicata+ Calabria + Campania + \n`Emilia Romagna` + `Friuli Venezia Giulia` + \nLazio + Liguria + Lombardia + Marche + Molise+\nPiemonte + Puglia + Sardegna + Sicilia +\nToscana + `Trentino alto Adige` + Umbria + \n`Valle d'Aosta` + Veneto, data = df)\n\nThe variable 'Veneto' has been removed because of collinearity (see $collin.var).\n\nsummary(lm_regione)\n\nOLS estimation, Dep. Var.: retric\nObservations: 138,618 \nStandard-errors: IID \n                          Estimate Std. Error    t value   Pr(&gt;|t|)    \n(Intercept)             1352.67843    5.88778 229.743479  &lt; 2.2e-16 ***\nAbruzzo                  -67.03866   11.67738  -5.740897 9.4372e-09 ***\nBasilicata               -62.22983   10.73230  -5.798367 6.7108e-09 ***\nCalabria                -159.50447   11.59058 -13.761558  &lt; 2.2e-16 ***\nCampania                -124.70487    8.18131 -15.242653  &lt; 2.2e-16 ***\n`Emilia Romagna`           8.80117    7.58301   1.160645 2.4579e-01    \n`Friuli Venezia Giulia`    3.25957    9.44556   0.345090 7.3003e-01    \nLazio                    -30.97062    7.82437  -3.958227 7.5546e-05 ***\nLiguria                   -2.50250   10.18666  -0.245665 8.0594e-01    \nLombardia                 56.13623    6.91684   8.115883 4.8617e-16 ***\nMarche                   -76.95157   10.14049  -7.588549 3.2551e-14 ***\nMolise                   -79.67389   14.50991  -5.490998 4.0037e-08 ***\nPiemonte                 -12.44023    7.66513  -1.622964 1.0460e-01    \nPuglia                  -139.24116    9.10816 -15.287521  &lt; 2.2e-16 ***\nSardegna                -150.99941   10.57937 -14.273005  &lt; 2.2e-16 ***\nSicilia                 -166.02165    7.95915 -20.859208  &lt; 2.2e-16 ***\nToscana                  -54.75509    8.10400  -6.756549 1.4187e-11 ***\n`Trentino alto Adige`     81.81903    7.95858  10.280604  &lt; 2.2e-16 ***\nUmbria                   -79.82247   10.44123  -7.644928 2.1040e-14 ***\n`Valle d'Aosta`           14.82207    9.45899   1.566983 1.1712e-01    \n... 1 variable was removed because of collinearity (Veneto)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 520.6   Adj. R2: 0.019842\n\n\nLa funzione feols ci informa che una delle variabili dummy è stata rimossa per evitare la collinearità (la trappola delle variabili dummy). I coefficienti della regressione sono la differenza fra il salario medio nella regione a cui il coefficiente si riferisce e la regione omessa dalla regressione che, in questo caso, è la regione Veneto. Per esempio, il coefficite della dummy Sicilia ci dice che in media i lavoratori residenti nell’isola guadagnano 166.0216453 euro in meno dei lavoratori residenti in Veneto.\nAggiungere variabili dummy al modello di regressione è un’operazione così frequente che R possiede un meccanismo automatico per la loro gestione: aggiungere nella regressione la variabile categorica automaticamente crea e aggiunge al modello una dummy per ciascuna categoria.\n\nfeols(retric~regione, data=df)\n\nOLS estimation, Dep. Var.: retric\nObservations: 138,618 \nStandard-errors: IID \n                               Estimate Std. Error    t value   Pr(&gt;|t|)    \n(Intercept)                  1285.63977    10.0844 127.487822  &lt; 2.2e-16 ***\nregioneBasilicata               4.80883    13.4986   0.356247 7.2166e-01    \nregioneCalabria               -92.46581    14.1905  -6.516024 7.2442e-11 ***\nregioneCampania               -57.66621    11.5743  -4.982284 6.2913e-07 ***\nregioneEmilia Romagna          75.83983    11.1594   6.796071 1.0794e-11 ***\nregioneFriuli Venezia Giulia   70.29823    12.4999   5.623892 1.8706e-08 ***\nregioneLazio                   36.06804    11.3248   3.184884 1.4484e-03 ** \nregioneLiguria                 64.53616    13.0689   4.938129 7.8967e-07 ***\n... 12 coefficients remaining (display them with summary() or use argument n)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 520.6   Adj. R2: 0.019842\n\n\nSe volessimo ottenere i risultati delle regressione omettando un’altra regione, possiamo usare la funzione i() che prende come pri argomento la variabile categorica e come secondo argomento la categoria da escludere. Pertanto, per escludere la dummy riferita alla regione Campania usiamo come regressore i(regione, \"Campanio\").\n\nfeols(retric~i(regione, \"Campania\"), data=df)\n\nOLS estimation, Dep. Var.: retric\nObservations: 138,618 \nStandard-errors: IID \n                                Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)                    1227.9736    5.68048 216.17407  &lt; 2.2e-16 ***\nregione::Abruzzo                 57.6662   11.57425   4.98228 6.2913e-07 ***\nregione::Basilicata              62.4750   10.62000   5.88277 4.0437e-09 ***\nregione::Calabria               -34.7996   11.48667  -3.02956 2.4495e-03 ** \nregione::Emilia Romagna         133.5060    7.42320  17.98496  &lt; 2.2e-16 ***\nregione::Friuli Venezia Giulia  127.9644    9.31776  13.73339  &lt; 2.2e-16 ***\nregione::Lazio                   93.7343    7.66960  12.22154  &lt; 2.2e-16 ***\nregione::Liguria                122.2024   10.06826  12.13738  &lt; 2.2e-16 ***\n... 12 coefficients remaining (display them with summary() or use argument n)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 520.6   Adj. R2: 0.019842\n\n\nCome nel caso della variabile genere, i valori dei coefficienti sono esattamenti uguali alle differenze delle medie calcolate manualmente. Per convincerci di ciò, possiamo calcolare la media di retric per regione e sottrarre la media della Campania usando la funzione group_by e summarize di dplyr.\n\n## Calcoliamo la media dei salari in Campania\nmc &lt;- df %&gt;% filter(regione==\"Campania\") %&gt;% \n  summarize(mc = mean(retric)) %&gt;% pull(mc)\n## `mc` e' uguale all'intercetta della regressione che \n## esclude la dummy per la Campania.\n## Calcoliamo la media dei salari in ciascuna regione e sottraiamo\n## `mc`\ndf %&gt;% filter(regione!=\"Campania\") %&gt;% \n  group_by(regione) %&gt;% \n  summarize(`Differenza da media Campania` = mean(retric)-mc) %&gt;%\n  arrange(`Differenza da media Campania`)\n\n# A tibble: 19 × 2\n   regione               `Differenza da media Campania`\n   &lt;chr&gt;                                          &lt;dbl&gt;\n 1 Sicilia                                        -41.3\n 2 Calabria                                       -34.8\n 3 Sardegna                                       -26.3\n 4 Puglia                                         -14.5\n 5 Umbria                                          44.9\n 6 Molise                                          45.0\n 7 Marche                                          47.8\n 8 Abruzzo                                         57.7\n 9 Basilicata                                      62.5\n10 Toscana                                         69.9\n11 Lazio                                           93.7\n12 Piemonte                                       112. \n13 Liguria                                        122. \n14 Veneto                                         125. \n15 Friuli Venezia Giulia                          128. \n16 Emilia Romagna                                 134. \n17 Valle d'Aosta                                  140. \n18 Lombardia                                      181. \n19 Trentino alto Adige                            207. \n\n\nIl vantaggio di usare la regressione rispetto a calcolare la differenza medie manualmente è la regressione ci restituisce gli errori standard delle differenze delle media consentendoci la costruzione immediata degli intervalli di confidenza.\n\nconfint(lm_regione)\n\n                              2.5 %      97.5 %\n(Intercept)             1341.138502 1364.218367\nAbruzzo                  -89.926111  -44.151208\nBasilicata               -83.264940  -41.194721\nCalabria                -182.221796 -136.787151\nCampania                -140.740079 -108.669653\n`Emilia Romagna`          -6.061372   23.663721\n`Friuli Venezia Giulia`  -15.253560   21.772697\nLazio                    -46.306226  -15.635005\nLiguria                  -22.468156   17.463149\nLombardia                 42.579363   69.693096\nMarche                   -96.826733  -57.076413\nMolise                  -108.113045  -51.234739\nPiemonte                 -27.463752    2.583285\nPuglia                  -157.092974 -121.389340\nSardegna                -171.734783 -130.264047\nSicilia                 -181.621436 -150.421854\nToscana                  -70.638786  -38.871401\n`Trentino alto Adige`     66.220357   97.417696\nUmbria                  -100.287085  -59.357850\n`Valle d'Aosta`           -3.717364   33.361506\n\n\n\n\nGender gap\nL’obiettivo è misurare la differenza salariale tra donne e uomini che condividono le stesse caratteristiche rilevanti, come l’istruzione, l’esperienza lavorativa, il settore di impiego, ecc., ma differiscono per genere.\n\nlm_gp &lt;- feols(retric~genere+regione, data=df)\nlm_gp\n\nOLS estimation, Dep. Var.: retric\nObservations: 138,618 \nStandard-errors: IID \n                                Estimate Std. Error    t value   Pr(&gt;|t|)    \n(Intercept)                  1118.134158    9.81249 113.950063  &lt; 2.2e-16 ***\ngenereMaschio                 290.437520    2.70004 107.568035  &lt; 2.2e-16 ***\nregioneBasilicata               0.001989   12.96823   0.000153 9.9988e-01    \nregioneCalabria               -91.809556   13.63291  -6.734407 1.6524e-11 ***\nregioneCampania               -65.965371   11.11971  -5.932295 2.9944e-09 ***\nregioneEmilia Romagna          95.899709   10.72248   8.943802  &lt; 2.2e-16 ***\nregioneFriuli Venezia Giulia   86.855842   12.00973   7.232126 4.7793e-13 ***\nregioneLazio                   48.195564   10.88033   4.429603 9.4479e-06 ***\n... 13 coefficients remaining (display them with summary() or use argument n)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 500.1   Adj. R2: 0.095359\n\n\nIn questa prima regressione stiamo “controllando” per la regione di residenza. L’interpretazione del coefficiente di gender è quindi la differenza media dei salari fra uomini e donne a parità di regione di residenza. La stima di tale differenza è di circa 290.44 euro. Questa differenza è statisticamente significativa, cioè possiamo rigettare l’ipotesi nulla che nella popolazione la differenza è uguale a zero (\\(H_0: \\beta_1=0\\)) poichè l’intervallo di onfidenza non contiene zero o, equivalentemente, la statistica \\(t\\) è maggiore in valore assoluto del valore critico (\\(1.96\\)).\nChiaramente, uomini e donne possono differire per livelli di istruzione (istruzione_anno), per la tipologia di contratto (detind) e se lavorano in part-time (piepar) e per il numero di ore lavorate.\n\nlm_gp2 &lt;- feols(retric~istruzione_anni+piepar+detind+genere+regione, data=df)\nlm_gp2\n\nOLS estimation, Dep. Var.: retric\nObservations: 138,618 \nStandard-errors: IID \n                            Estimate Std. Error    t value   Pr(&gt;|t|)    \n(Intercept)               -19.714271   8.840365  -2.230029 2.5747e-02 *  \nistruzione_anni            41.355406   0.261081 158.400638  &lt; 2.2e-16 ***\npiepartempo pieno         521.552688   2.792073 186.797681  &lt; 2.2e-16 ***\ndetindtempo indeterminato 286.402687   2.939087  97.446140  &lt; 2.2e-16 ***\ngenereMaschio             201.536591   2.283682  88.250741  &lt; 2.2e-16 ***\nregioneBasilicata          -0.101848  10.224505  -0.009961 9.9205e-01    \nregioneCalabria           -39.440407  10.749966  -3.668887 2.4370e-04 ***\nregioneCampania           -42.821891   8.767499  -4.884163 1.0398e-06 ***\n... 16 coefficients remaining (display them with summary() or use argument n)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 394.3   Adj. R2: 0.437681\n\n\nPer tenere conto delle ore lavorate (orelav) possiamo usare come variabile dipendente il rapporto fra il salario orario netto mensile e le ore lavorete nel mese (che è uguale a \\(4\\times orelav\\)):\n\nlm_gp3 &lt;- feols(I(retric/(4*orelav))~istruzione_anni+piepar+detind+genere+regione, data=df)\nlm_gp3\n\nOLS estimation, Dep. Var.: I(retric/(4 * orelav))\nObservations: 138,618 \nStandard-errors: IID \n                           Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)                3.724961   0.201074 18.525280  &lt; 2.2e-16 ***\nistruzione_anni            0.403373   0.005938 67.927402  &lt; 2.2e-16 ***\npiepartempo pieno         -0.178177   0.063506 -2.805674 5.0218e-03 ** \ndetindtempo indeterminato  1.799457   0.066850 26.917971  &lt; 2.2e-16 ***\ngenereMaschio              0.287500   0.051942  5.534970 3.1184e-08 ***\nregioneBasilicata         -0.158508   0.232557 -0.681588 4.9550e-01    \nregioneCalabria           -0.462131   0.244508 -1.890040 5.8755e-02 .  \nregioneCampania           -0.312879   0.199417 -1.568968 1.1666e-01    \n... 16 coefficients remaining (display them with summary() or use argument n)\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 8.96792   Adj. R2: 0.040316\n\n\n\nEsercizi:\n\nCome possiamo interpretare il coefficiente su genereMaschio nell’ultima regressione? Per esempio, come è possibile interpretare la grandezza del coefficiente in maniera percentuale (per esemptio, di quanto è più alto il salario degli uomini in termini percentuali rispetto a quello delle donne?)\nLa differenza fra i salari fra uomini e donne nell’ultima regression è statisticamente significativa?\nQuali sono le altre variabili che è probabile possono influenzare l’interpretezione del coefficiente su genereMaschio come indicativo di discriminazione?",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili dummy"
    ]
  },
  {
    "objectID": "IVapp.html",
    "href": "IVapp.html",
    "title": "Variabili Strumentali",
    "section": "",
    "text": "Una stima precisa dell’effetto dell’educazione sui salari è importante sia a livello individuale che sociale. A livello individuale, comprendere il ritorno sull’investimento in istruzione aiuta le persone a fare scelte informate riguardo al loro percorso educativo. Questo è particolarmente rilevante in contesti in cui l’educazione rappresenta un investimento significativo di tempo e risorse finanziarie. A livello sociale, una stima precisa degli effetti dell’educazione sui salari fornisce ai policymaker un’informazione cruciale per formulare politiche di efficaci. Ad esempio, politiche che mirano a incrementare l’accesso all’istruzione superiore potrebbero essere giustificate da un alto ritorno economico dell’educazione, il che a sua volta può stimolare la crescita economica, ridurre la disuguaglianza e promuovere una maggiore coesione sociale.",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili strumentali"
    ]
  },
  {
    "objectID": "IVapp.html#ritorni-di-educazione",
    "href": "IVapp.html#ritorni-di-educazione",
    "title": "Variabili Strumentali",
    "section": "",
    "text": "Una stima precisa dell’effetto dell’educazione sui salari è importante sia a livello individuale che sociale. A livello individuale, comprendere il ritorno sull’investimento in istruzione aiuta le persone a fare scelte informate riguardo al loro percorso educativo. Questo è particolarmente rilevante in contesti in cui l’educazione rappresenta un investimento significativo di tempo e risorse finanziarie. A livello sociale, una stima precisa degli effetti dell’educazione sui salari fornisce ai policymaker un’informazione cruciale per formulare politiche di efficaci. Ad esempio, politiche che mirano a incrementare l’accesso all’istruzione superiore potrebbero essere giustificate da un alto ritorno economico dell’educazione, il che a sua volta può stimolare la crescita economica, ridurre la disuguaglianza e promuovere una maggiore coesione sociale.",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili strumentali"
    ]
  },
  {
    "objectID": "IVapp.html#descrizione-del-dataset-card",
    "href": "IVapp.html#descrizione-del-dataset-card",
    "title": "Variabili Strumentali",
    "section": "Descrizione del Dataset card",
    "text": "Descrizione del Dataset card\n\nlibrary(wooldridge)\ndata(card)\n\n\n#echo: false\nlibrary(formattable)\n\nIl dataset card include una serie di variabili relative alle caratteristiche demografiche degli individui, come età, genere, il livello di istruzione raggiunto, l’esperienza lavorativa, e altri fattori che possono influenzare i salari. Useremo questo dataset per applicare la tecnica delle variabili strumentali per progvare ad isolare l’effetto dell’istruzione dalle distorsioni dovute a variabili omesse.\nLa ?@tbl-descrizione contiene una descrizione dettagliata delle variabile contenute nel dataset. La Table 1 contiene invece la statistiche descrittive.\n\n\n\n\n\n\n\nVariabile\nDescrizione\n\n\n\n\nid\nIdentificativo unico per ciascun individuo\n\n\nnearc2\n(1/0) se vicino a un college a 2 anni\n\n\nnearc4\n(1/0) se vicino a un college a 4 anni\n\n\neduc\nAnni di educazione completati\n\n\nage\nEtà dell’individuo\n\n\nfatheduc\nAnni di educazione completati dal padre\n\n\nmotheduc\nAnni di educazione completati dalla madre\n\n\nweight\nPeso campionario\n\n\nmomdad14\n(1/0) se viveva con entrambi i genitori a 14 anni\n\n\nsinmom14\n(1/0) se viveva solo con la madre a 14 anni\n\n\nstep14\n(1/0) se viveva con un genitore acquisito a 14 anni\n\n\nblack\n(1/0) se l’individuo è di razza nera\n\n\nsmsa\n(1/0) se residente in un’area metropolitana\n\n\nsouth\n(1/0) se residente nel sud degli USA\n\n\nwage\nSalario annuo\n\n\nenroll\n(1/0) di iscrizione a scuola nel 1966\n\n\nKWW\nPunteggio sul test di conoscenza del mondo del lavoro\n\n\nIQ\nQuoziente intellettivo\n\n\nmarried\n(1/0) dello stato civile (sposato)\n\n\nexper\nAnni di esperienza lavorativa\n\n\nlwage\nLogaritmo naturale del salario\n\n\nexpersq\nEsperienza lavorativa al quadrato\n\n\n\n\nlibrary(\"gtsummary\")\n\ncard |&gt;\n  tbl_summary(\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} ({p}%)\"\n    ),\n    digits = all_continuous() ~ 2,\n    missing_text = \"(Osservazioni mancanti)\"\n    #include = names(Hmda)\n  ) |&gt;\n  modify_header(label = \"**Variabili**\") |&gt;\n  bold_labels() #|&gt;\n  #add_n() # add column with total number of non-missing observations\n\n\n\nTable 1: Dataset card: statistiche descrittive.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariabili\nN = 3,0101\n\n\n\n\nid\n2,581.75 (1,500.54)\n\n\nnearc2\n1,327 (44%)\n\n\nnearc4\n2,053 (68%)\n\n\neduc\n13.26 (2.68)\n\n\nage\n28.12 (3.14)\n\n\nfatheduc\n10.00 (3.72)\n\n\n    (Osservazioni mancanti)\n690\n\n\nmotheduc\n10.35 (3.18)\n\n\n    (Osservazioni mancanti)\n353\n\n\nweight\n321,185.26 (170,645.80)\n\n\nmomdad14\n2,376 (79%)\n\n\nsinmom14\n303 (10%)\n\n\nstep14\n117 (3.9%)\n\n\nreg661\n140 (4.7%)\n\n\nreg662\n484 (16%)\n\n\nreg663\n589 (20%)\n\n\nreg664\n193 (6.4%)\n\n\nreg665\n627 (21%)\n\n\nreg666\n289 (9.6%)\n\n\nreg667\n331 (11%)\n\n\nreg668\n85 (2.8%)\n\n\nreg669\n272 (9.0%)\n\n\nsouth66\n1,247 (41%)\n\n\nblack\n703 (23%)\n\n\nsmsa\n2,146 (71%)\n\n\nsouth\n1,215 (40%)\n\n\nsmsa66\n1,955 (65%)\n\n\nwage\n577.28 (262.96)\n\n\nenroll\n278 (9.2%)\n\n\nKWW\n33.54 (8.61)\n\n\n    (Osservazioni mancanti)\n47\n\n\nIQ\n102.45 (15.42)\n\n\n    (Osservazioni mancanti)\n949\n\n\nmarried\n\n\n\n\n    1\n2,144 (71%)\n\n\n    2\n14 (0.5%)\n\n\n    3\n3 (&lt;0.1%)\n\n\n    4\n155 (5.2%)\n\n\n    5\n102 (3.4%)\n\n\n    6\n585 (19%)\n\n\n    (Osservazioni mancanti)\n7\n\n\nlibcrd14\n2,021 (67%)\n\n\n    (Osservazioni mancanti)\n13\n\n\nexper\n8.86 (4.14)\n\n\nlwage\n6.26 (0.44)\n\n\nexpersq\n95.58 (84.62)\n\n\n\n1 Mean (SD); n (%)",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili strumentali"
    ]
  },
  {
    "objectID": "IVapp.html#stima-ols-di-base",
    "href": "IVapp.html#stima-ols-di-base",
    "title": "Variabili Strumentali",
    "section": "Stima OLS di Base",
    "text": "Stima OLS di Base\nIniziamo con il semplice modello linear che include soltanto la variabile educ \\[\n\\log(wage_i) = \\beta_0 + \\beta_1 educ_i + u_i\n\\]\nStimiamo il modello per mezzo dei minimi quadrati ordinari (OLS) usando la funzione feols del pacchetto fixest:\n\nlibrary(fixest)\nlm1 &lt;- feols(log(wage) ~ educ, data=card, vcov=\"hetero\")\nlm1\n\nOLS estimation, Dep. Var.: log(wage)\nObservations: 3,010\nStandard-errors: Heteroskedasticity-robust \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 5.570882   0.039093 142.5016 &lt; 2.2e-16 ***\neduc        0.052094   0.002907  17.9210 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.421249   Adj. R2: 0.098437\n\n\nIl coefficiente stimato è \\(hat{\\beta}_1=0.0521\\). Questa stima è quasi sicuramente distorta per via delle variabili omesse cioè quelle variabili che hanno influire sia sul salario sia sul livello di istruzione. La presenza di queste varibili rende l’assunzione \\(E(u_i|educ_i)=0\\) poco credibile e, di conseguenza, un’interpretazione causale del coefficiente è impossibile.\nPossiamo in ogni caso interpretatare il coefficiente stimato come indicatore dell’associazione statistica esistente fra i salari degli individui e il loro livello di istruzione. In particolare, possiamo dire che un anno addizionale di istruzione è associato con una crescita salariale del 5.21%.",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili strumentali"
    ]
  },
  {
    "objectID": "IVapp.html#modello-ols-con-variabili-esogene",
    "href": "IVapp.html#modello-ols-con-variabili-esogene",
    "title": "Variabili Strumentali",
    "section": "Modello OLS con variabili esogene",
    "text": "Modello OLS con variabili esogene\nPer attenuare il problema delle variabili omesse possiamo includere nel modello delle variabili che pensiamo possano essere fonte di distorsione e consideriamo il seguente modello:\n\\[\n\\log(wage_i) = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 exper_i^2 + \\beta_4 south_i + \\beta_5smsa_i + \\beta_6 black + u_i.\n\\tag{1}\\]\n\nlm2 &lt;- feols(log(wage) ~ educ + exper + I(exper^2) + south + smsa + black, data=card, vcov=\"hetero\")\nlm2\n\nOLS estimation, Dep. Var.: log(wage)\nObservations: 3,010\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept)  4.733664   0.070158  67.47181  &lt; 2.2e-16 ***\neduc         0.074009   0.003642  20.32079  &lt; 2.2e-16 ***\nexper        0.083596   0.006733  12.41654  &lt; 2.2e-16 ***\nI(exper^2)  -0.002241   0.000318  -7.04429 2.3017e-12 ***\nsouth       -0.124862   0.015351  -8.13390 6.0179e-16 ***\nsmsa         0.161423   0.015175  10.63736  &lt; 2.2e-16 ***\nblack       -0.189632   0.017432 -10.87809  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.373755   Adj. R2: 0.289088\n\n\nL’inclusione di esperienza lavorativa (anche in forma quadratica per catturare effetti non lineari), la regione geografica (south), l’area metropolitana e black fornisce un controllo più accurato di altri fattori che potrebbero influenzare i salari. In questo nuovo modello, un anno addizionale di istruzione è associato con una crescita salariale del 7.40%.\nPer poter interpretatare questa stima come causale, dovremmo essere certi che il nuovo errore non contenga più nessuna variabile che simultaneamente determini il salario ed una delle altre variabili che abbiamo introdotto nel modello. In altre parole, dovremmo essere pronti a sostenare l’assunzione \\[\nE(u_i|educ_i, exper_i, south, smsa_i, black_i) = 0.\n\\tag{2}\\]\nLa variabile che pensiamo possa ancora essere nell’errore è \\(q_i\\), l’abilità dell’individuo. Questa variabile non osservabile include tratti come la motivazione personale, la resilienza, le abilità sociali, non è osservabile che possono influenzare il livello del salario di un lavoratore.\nRicordiamo che l’esclusione di questa variabile dal modello Equation 2 risulterà problematica se \\[\nE(q_i|educ_i, exper_i, south, smsa_i, black_i) \\neq 0.\n\\] Siamo pronti a considerare \\(exper_i, south, smsa_i, black_i\\) come variabile esogene, cioè variabili che non hanno una relazione sistematica con quelle varibili: le persone che abitano nel sud (south=1) o in un’area metropolitana (smsa=1) non hanno un’abilità superiore ai lavoratori del nord o che vivono in area rurali. Anche per quanto riguarda l’esperienza e il colore della pelle siamo più che pronti ad assumere che non ci sia una sistematica relazione con \\(q_i\\). Quindi possiamo sostenere a testa alta \\[\nE(q_i|exper_i, south, smsa_i, black_i)=0,\n\\tag{3}\\] e che quindi l’errore (che contiene \\(q_i\\)) non sia sistematicamente associato con \\(exper_i, south, smsa_i, black_i\\). In gergo, diciamo che queste variabili sono esogene. Rimane il problema dell’istruzione. Quasi per devinizione, l’abilità individuale determina il livello di istruzione di un individuo: la motivazione personale, la diligenza e la capacità di perseverare nelle difficoltà sono chiaramente associate con una probabilità più alta di completare un percorso di studi più avanzato. Il risultato è una relazione fra \\(q_i\\) e \\(educ_i\\) quindi, sebbena Equation 3 possa essere assunta come valida, non possiamo assumere soddisfatta Equation 2.\n\nVariabili strumentali\nDavid Card ha proposto di “strumentare” i livelli di istruzione usando la prossimità geografica ad una università. Le variabili in questione sono nearc2 and nearc4: variabili binari che prendono il valore \\(1\\) se l’individuo risiedeva nel 1966 (le altre variabili si riferiscono al 1976) vicino ad un college di due anni o ad un college di quattro anni.\n\n\nGiustificazione della validità degli strumenti\nPer giustificare l’uso di nearc2 e nearc4 come strumenti per educ è necessario verificare che due condizioni siano soddisfatte:\n\nRilevanza: Gli strumenti devono essere correlati, vedremo meglio in che modo, con la variabile endogena (educ). L’ipotesi è che la prossimità a un istituto di istruzione superiore abbassi i costi dell’istruzione, sia diretti che indiretti, e quindi incrementi la probabilità che un individuo prosegua l’educazione oltre il livello secondario.\nEsogeneità: Lo strumento non deve essere correlato con l’errore, cioè con l’abilità. In altre parole, la prossimità a un college influenzare i salari degli individui solo attraverso il suo effetto sull’istruzione, e non dovrebbe essere direttamente correlata con altri fattori che influenzano i salari, come l’abilità non osservata o le preferenze lavorative. Vedremo che questa assunzione può essere discutibile, ma per il momento procediamo assumendo che gli strumenti siano endogeni.",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili strumentali"
    ]
  },
  {
    "objectID": "IVapp.html#rilevanza",
    "href": "IVapp.html#rilevanza",
    "title": "Variabili Strumentali",
    "section": "Rilevanza",
    "text": "Rilevanza\nIl modo più semplice, ma anche impreciso, di testare la rilevanza degli strumenti è quello di calcolare la correlazione fra educ e nearc2 e nearc4. Questo può essere fatto usando il modello linear, visto che nel modello univariato il coefficiente di regressione è il rapporto fra la covarianza fra le variabili e la varianza della variabile dipendente. Se c’è correlazione allora il coefficiente deve essere (significativamente) diverso da zero.\n\nsummary(feols(educ ~ nearc2, data=card, vcov = \"hetero\"))\n\nOLS estimation, Dep. Var.: educ\nObservations: 3,010\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value  Pr(&gt;|t|)    \n(Intercept) 13.150921   0.064510 203.85880 &lt; 2.2e-16 ***\nnearc2       0.255258   0.098453   2.59269  0.009569 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 2.67347   Adj. R2: 0.00191\n\n\n\nsummary(feols(educ ~ nearc4, data=card, vcov = \"hetero\"))\n\nOLS estimation, Dep. Var.: educ\nObservations: 3,010\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept) 12.698015   0.090220 140.74510  &lt; 2.2e-16 ***\nnearc4       0.829019   0.106694   7.77005 1.0684e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 2.64848   Adj. R2: 0.02048\n\n\nIn entrambi i casi, il coefficiente è positivo: esiste un’associazione positiva fra vivere vicino ad un college e il livello di istruzione raggiunto. I coefficienti sono statisticamente significativi: le statistiche \\(t\\), ottenuti mediante usando uno standard error valido anche in presenza di eteroschedasticità, sono entrambi superiori al livello critico al livello di significatività \\(\\alpha=0.05\\). I due strumenti mostrano però una diversa “forza”: la correlazione fra nearc2 e educ è \\(\\sqrt{R^2} = 0.047\\) mentre è di \\(\\sqrt{R^2} = 0.144\\) fra nearc4 e `educ1.\nL’esistenza di una semplice correlazione non è comunque evidenza sufficiente a caratterizzare i nostri strumenti come rilveanti. Il modello in Equation 1 ha le seguente form: \\[\ny_i = \\beta_0 + \\beta_1 X_{i} + \\beta_2 W_{1i} + \\beta_3 W_{2i} + \\dots + \\beta_{r+1}W_{ri} + u_i,\n\\] dove \\(X_i = educ_i\\) è la variabile _endogena` e \\(W_{1i} = exper_i\\), \\(W_{2i} = exper_i^2\\), \\(W_{3i} = south_i\\), \\(W_{4i} = smsa_i\\) e \\(W_{5i} = black_i\\) sono le variabili esogene.\nIn questo modello la condizione di rilevanza è che il coefficiente sullo strumento in una regressione fra la variabile endogena, lo strumento e le variabili esogene sia statisticamente diverso da zero. Nel caso volessimo usare entrambi gli strumenti, dobbiamo testare l’ipotesi nulla che i coefficienti di entrambi gli strumenti siano uguali a zero.\n\nfirst_stage_nearc2 &lt;- feols(educ ~ nearc2 + exper + I(exper^2) + south + smsa + black, data=card, vcov = \"hetero\")\nfirst_stage_nearc4 &lt;- feols(educ ~ nearc4 + exper + I(exper^2) + south + smsa + black, data=card, vcov = \"hetero\")\nfirst_stage_nearc4_2 &lt;- feols(educ ~ nearc4 + nearc2 + exper + I(exper^2) + south + smsa + black, data=card, vcov = \"hetero\")\netable(first_stage_nearc2, first_stage_nearc4, first_stage_nearc4_2, highlight = list(m1 = \"nearc2\", m2 = \"nearc4\", m1 = c(\"nearc2\", \"nearc4\")), order = c(\"nearc2\", \"nearc4\", \"exper\", \"exper square\", \"south\", \"smsa\", \"black\"))\n\n                 first_stage_nearc2  first_stage_nearc4 first_stage_near..2\nDependent Var.:                educ                educ                educ\n                                                                           \nnearc2             0.1222. (0.0735)                         0.1077 (0.0731)\nnearc4                               0.3373*** (0.0806)  0.3312*** (0.0806)\nexper square        0.0007 (0.0017)     0.0007 (0.0017)     0.0007 (0.0017)\nexper           -0.4096*** (0.0319) -0.4100*** (0.0320) -0.4095*** (0.0319)\nsouth           -0.3269*** (0.0787) -0.2915*** (0.0785) -0.2787*** (0.0788)\nsmsa             0.4969*** (0.0816)  0.4039*** (0.0849)  0.3887*** (0.0857)\nblack            -1.017*** (0.0880)  -1.006*** (0.0878)  -1.013*** (0.0877)\nConstant          16.79*** (0.1428)   16.66*** (0.1467)   16.62*** (0.1495)\n_______________ ___________________ ___________________ ___________________\nS.E. type       Heteroskedast.-rob. Heteroskedast.-rob. Heteroskedast.-rob.\nObservations                  3,010               3,010               3,010\nR2                          0.47203             0.47447             0.47485\nAdj. R2                     0.47098             0.47342             0.47362\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIl coefficiente di nearc2 nella regressione riportata nella prima colonna è statisticamente diverso da zero al 10% di significatività (\\(t=1.66 &gt; 1.64\\)). Il coefficiente di nearc4 nella regressione riportata nella secondo colonna è statisticamente diverso da zero allo 0.1% di significatività (\\(t=4.18 &gt; 3.29\\)). Per verificare se entrambi i coefficienti della regressione della terza colonna sono entrambi diversi da zero dobbiamo calcolare la statistica di Wald:\n\nlibrary(car)\n\nLoading required package: carData\n\njoint_test &lt;- linearHypothesis(first_stage_nearc4_2, c(\"nearc2=0\", \"nearc4=0\"), test = \"Chisq\")\njoint_test\n\nLinear hypothesis test\n\nHypothesis:\nnearc2 = 0\nnearc4 = 0\n\nModel 1: restricted model\nModel 2: educ ~ nearc4 + nearc2 + exper + I(exper^2) + south + smsa + \n    black\n\n  Res.Df Df  Chisq Pr(&gt;Chisq)    \n1   3004                         \n2   3002  2 19.433  6.026e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRigettiamo l’ipotesi nulla che entrambi i coefficienti siano simultanemeante uguali a zero.[^chisq]\n[chisq:] Il test di Wald ha una distribuzione \\(\\chi^2\\) con un numero di gradi di libertà pari al numero dei coefficienti che stiamo testando. In questo caso, i gradi di libertà saranno quindi \\(df=2\\). Il valore critico della \\(\\chi_2^2\\) è \\(5.99\\) che è minore del valore del test (\\(19.43\\)). Alternativamente, avremmo potuto usare il \\(p\\)-value. Il \\(p\\)-value è \\(6.0264295\\times 10^{-5}\\) che essendo più piccolo di qualsiasi ragionevole livello di significatività induce a rigettare l’ipotesi nulla.",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili strumentali"
    ]
  },
  {
    "objectID": "IVapp.html#secondo-stadio-della-regressione-iv",
    "href": "IVapp.html#secondo-stadio-della-regressione-iv",
    "title": "Variabili Strumentali",
    "section": "Secondo Stadio della Regressione IV",
    "text": "Secondo Stadio della Regressione IV\nLe regressioni della precedente sezione costituiscono il primo stadio nella costruzione dello stimatore TSLS. Nel secondo stadio andiamo a regredire la variabile educ predetta dalle regressioni del primo stadio nei tre casi:\n\neduc_hat_nearc2 &lt;- predict(first_stage_nearc2)\neduc_hat_nearc4 &lt;- predict(first_stage_nearc4)\neduc_hat_nearc4_2 &lt;- predict(first_stage_nearc4_2)\n## Includiamo questi valori predetti nel data.frame `card`\n## affinchè siano accessibili da `feols`\ncard$educ_hat_nearc2 &lt;- educ_hat_nearc2\ncard$educ_hat_nearc4 &lt;- educ_hat_nearc4\ncard$educ_hat_nearc4_2 &lt;- educ_hat_nearc4_2\n\n\nsecond_stage_nearc2 &lt;- feols(log(wage) ~ educ_hat_nearc2 + exper + I(exper^2) + south + smsa + black, data=card, vcov = \"hetero\")\nsecond_stage_nearc4 &lt;- feols(log(wage) ~ educ_hat_nearc4 + exper + I(exper^2) + south + smsa + black, data=card, vcov = \"hetero\")\nsecond_stage_nearc4_2 &lt;- feols(log(wage) ~ educ_hat_nearc4_2 + exper + I(exper^2) + south + smsa + black, data=card, vcov = \"hetero\")\netable(second_stage_nearc2, second_stage_nearc4, second_stage_nearc4_2, \n       highlight = list(m1 = \"educ_hat_nearc2\", \n                        m2 = \"educ_hat_nearc4\", \n                        m1 = c(\"educ_hat_nearc2\", \n                               \"educ_hat_nearc4\", \n                               \"educ_hat_nearc4_2\")), \n       order = c(\"educ_hat_nearc2\", \"educ_hat_nearc4\", \n                 \"exper\", \"exper square\", \"south\", \"smsa\", \"black\"))\n\n                  second_stage_nearc2 second_stage_nearc4 second_stage_nea..2\nDependent Var.:             log(wage)           log(wage)           log(wage)\n                                                                             \neduc_hat_nearc2     0.3498** (0.1237)                                        \neduc_hat_nearc4                         0.1323** (0.0486)                    \neduc_hat_nearc4_2                                          0.1608*** (0.0461)\nexper square      -0.0024*** (0.0003) -0.0023*** (0.0003) -0.0023*** (0.0003)\nexper              0.1967*** (0.0513)  0.1075*** (0.0211)  0.1192*** (0.0202)\nsouth                -0.0304 (0.0458) -0.1049*** (0.0233) -0.0951*** (0.0228)\nsmsa                  0.0190 (0.0651)  0.1313*** (0.0294)  0.1166*** (0.0281)\nblack                 0.0887 (0.1262)   -0.1308* (0.0519)   -0.1020* (0.0496)\nConstant               0.0926 (2.083)   3.753*** (0.8180)   3.272*** (0.7771)\n_________________ ___________________ ___________________ ___________________\nS.E. type         Heteroskedast.-rob. Heteroskedast.-rob. Heteroskedast.-rob.\nObservations                    3,010               3,010               3,010\nR2                            0.18739             0.18706             0.18831\nAdj. R2                       0.18576             0.18543             0.18668\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUn problem per l’interpretazione della tavola precedente è che gli errori standard non sono corretti. Per ottenere quelli corretti dobbiamo utilizzare la funzione feols in maniera che i due stadi siano calcolati internamente cosicchè sia possibile aggiunstare gli errori standard in maniera tale che tengano conto del fatto che le variabili educ_hat_nearc2 e educ_hat_nearc4 sono le predizioni dei primi stadi.\n\ntsls_nearc2 &lt;- feols(log(wage) ~ exper + I(exper^2) + south + smsa + black \n                     | educ ~ nearc2, data=card, vcov = \"hetero\")\ntsls_nearc4 &lt;- feols(log(wage) ~ exper + I(exper^2) + south + smsa + black \n                     | educ ~ nearc4, data=card, vcov = \"hetero\")\ntsls_nearc4_2 &lt;- feols(log(wage) ~ exper + I(exper^2) + south + smsa + black \n                       | educ ~ nearc2+nearc4, data=card, vcov = \"hetero\")\netable(tsls_nearc2, tsls_nearc4, tsls_nearc4_2, \n       highlight = \"educ\", \n       order = c(\"educ\", \"exper\", \"exper square\", \"south\", \"smsa\", \"black\"))\n\n                        tsls_nearc2         tsls_nearc4       tsls_nearc4_2\nDependent Var.:           log(wage)           log(wage)           log(wage)\n                                                                           \neduc               0.3498. (0.2023)   0.1323** (0.0486)  0.1608*** (0.0486)\nexper square    -0.0024*** (0.0006) -0.0023*** (0.0003) -0.0023*** (0.0004)\nexper              0.1967* (0.0842)  0.1075*** (0.0211)  0.1192*** (0.0213)\nsouth              -0.0304 (0.0738) -0.1049*** (0.0229) -0.0951*** (0.0234)\nsmsa                0.0190 (0.1084)  0.1313*** (0.0298)  0.1166*** (0.0303)\nblack               0.0887 (0.2079)   -0.1308* (0.0515)   -0.1020. (0.0521)\nConstant             0.0926 (3.406)   3.753*** (0.8177)   3.272*** (0.8178)\n_______________ ___________________ ___________________ ___________________\nS.E. type       Heteroskedast.-rob. Heteroskedast.-rob. Heteroskedast.-rob.\nObservations                  3,010               3,010               3,010\nR2                          -1.1715             0.22520             0.14551\nAdj. R2                     -1.1759             0.22365             0.14381\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI coefficienti stimati da questa ultima serie di comandi e riportati nella tabella sono identici a quelli ottenuti alla procedura a due stadi manuale. La differenza consiste negli errori standard che sono diversi da quelli ottenuti dalla prima procedura.\nL’analisi delle stime OLS e TSLS basata sulle tabelle fornite offre una visione approfondita di come i due metodi trattano l’endogeneità dell’istruzione nei modelli di salario. I dati mostrano come gli stimatori TSLS correggano il bias che affligge le stime OLS a causa delle variabili omesse o della correlazione tra le variabili esplicative e il termine di errore.\nRiassumendo i nostri risultati:\n\nRisultati OLS\n\nModello OLS Semplice (lm1): Questo modello stima che ogni anno aggiuntivo di istruzione è associato ad una crescita del salario del 5.2%.\nModello OLS Esteso (lm2): Introducendo controlli per l’esperienza, l’esperienza quadrata, la regione (south), l’area metropolitana (smsa), e la razza (black), il coefficiente dell’istruzione passa a 0.0740 (aumento del 7.4% del salario per ogni anno di istruzione addizionale).\n\n\n\nRisultati TSLS\n\nModello TSLS con nearc2 (tsls_nearc2): L’uso di nearc2 come strumento per l’istruzione mostra un coefficiente molto più alto per l’istruzione (35%). Questo coefficiente non è statisticamente significativo al livello convenzionale. L’imprecisone è probabilmente dovuta al fatto che questo strumento sebbene soddisfi la condizione di rilevanza, non ha una “forte” correlazione con educ.\nModello TSLS con nearc4 (tsls_nearc4): Il coefficiente per l’istruzione è 0.1323, statisticamente significativo al livello del 1%.\nModello TSLS con nearc2 e nearc4 (tsls_nearc4_2): L’uso combinato di entrambi gli strumenti produce un coefficiente di 0.1608, anch’esso significativo e indicativo di un ritorno dell’istruzione più elevato rispetto all’OLS esteso.\n\nI risultati TSLS indicano che, quando si corregge per l’endogeneità utilizzando variabili strumentali appropriate, i ritorni stimati dell’istruzione sui salari sono significativamente più alti rispetto alle stime OLS, che possono soffrire di bias dovuto a variabili omesse o misurate in modo errato.",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili strumentali"
    ]
  },
  {
    "objectID": "IVapp.html#test-per-endogeneità",
    "href": "IVapp.html#test-per-endogeneità",
    "title": "Variabili Strumentali",
    "section": "Test per endogeneità",
    "text": "Test per endogeneità\nIl test di endogeneità, noto anche come test J è un test usato per verificare se gli strumenti utilizzati in un modello con variabili strumentali sono esogeni.\nLa procedura inizia con la stima dell’equazione di interesse tramite TSLS, utilizzando tutti gli strumenti disponibili. Usando questa regressione, vengono calcolati i residui\n\nu_hat &lt;- resid(tsls_nearc4_2)\ncard$u_hat &lt;- u_hat\n\nQuesti residui vengono utilizzati come variabile dipendente in una nuova regressione, dove le variabili indipendenti includono sia gli strumenti (nearc2 e nearc4) sia le altre variabili esogene del modello originale (exper, exper^2, south, smsa, black).\n\njreg &lt;- feols(u_hat~nearc2 + nearc4+exper+exper^2+\n                south+smsa+black, data = card, \n              vcov = \"hetero\")\n\nIl cuore del test è il test Wald per verificare se i coefficienti associati agli strumenti siano statisticamente diversi da zero.\n\nlinearHypothesis(jreg, c(\"nearc2=0\", \"nearc4=0\"), test=\"Chisq\")\n\nLinear hypothesis test\n\nHypothesis:\nnearc2 = 0\nnearc4 = 0\n\nModel 1: restricted model\nModel 2: u_hat ~ nearc2 + nearc4 + exper + I(exper^2) + south + smsa + \n    black\n\n  Res.Df Df  Chisq Pr(&gt;Chisq)\n1   3004                     \n2   3002  2 2.6552     0.2651\n\n\nPer analizzare il risultato del test, bisogna tenere in mente che i gradi di libertà di questo test non corrispondono al numero delle restrizioni testate (nel caso specifico, due: nearc2 = 0 e nearc4 = 0), ma piuttosto al numero di strumenti (2) meno il numero di variabili endogene (1). In questo esempio, la statistica chi-quadrato risultante è \\(2.6552\\), che deve essere confrontata con il valore critico della distribuzione chi-quadrato con un grado di libertà (3.84). Quindi, visto 2.6552 &lt; 3.84, non possiamo rigettare l’ipotesi nulla che entrambi i coefficienti siano uguali a zero. Il \\(J\\) test suggerisce quindi l’esogeneità degli strumenti.",
    "crumbs": [
      "Syllabus",
      "R",
      "Variabili strumentali"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometria (1018133)",
    "section": "",
    "text": "Prof. Giuseppe Ragusa\n   Sapienza, Università di Roma\n   Dipartimento di Economia e Diritto\n   Viale del Castro Laurenziano, 9\n   giuseppe.ragusa at uniroma1 dot it\n   Ricevimento\n\n\n\n\n\n   Lunedì (L) e Martedì (M)\n   19 febbraio, 2024 - 1 giugno 2024\n   (L) 10:00–12:00 (M) 14:00-16:00\n   (L) Aula 10 (M) Aula 8b\n   Informazione Esame"
  },
  {
    "objectID": "index.html#descrizione-del-corso",
    "href": "index.html#descrizione-del-corso",
    "title": "Econometria (1018133)",
    "section": "Descrizione del corso",
    "text": "Descrizione del corso\nEconometria (1018133) offre agli studenti un’introduzione pratica all’econometria, strumento fondamentale per comprendere e analizzare l’economia da un punto di vista empirico. Attraverso un approccio equilibrato tra teoria e pratica, gli studenti esploreranno i concetti fondamentali e gli strumenti empirici necessari per comprendere le sfide relative all’impiego di tecniche quantitative in microeconomia, macroeconomia e finanza."
  },
  {
    "objectID": "materiale.html",
    "href": "materiale.html",
    "title": "Materiale didattico",
    "section": "",
    "text": "Questa pagina contiene i collegamenti al materiale didattico del corso. (Nella sidebar qui a sinistra)\nIl materiale didattico è organizzato per lezione e a ciscuna lezione è associato\n\nle slide \ndispense R in formato html \nil codice R in formato Rmd in formato Rmd \nletture consigliate \n\nI collegamenti (a sinistra) si attiveranno in sincrono con la progressione del corso."
  },
  {
    "objectID": "probmodel_app.html",
    "href": "probmodel_app.html",
    "title": "Probit e Logit",
    "section": "",
    "text": "Questo documento descrive l’analisi del dataset Hmda, che contiene dati relativi alle domande di mutuo e le relative approvazioni o rifiuti.",
    "crumbs": [
      "Syllabus",
      "R",
      "Modelli per probabilità"
    ]
  },
  {
    "objectID": "probmodel_app.html#introduzione",
    "href": "probmodel_app.html#introduzione",
    "title": "Probit e Logit",
    "section": "",
    "text": "Questo documento descrive l’analisi del dataset Hmda, che contiene dati relativi alle domande di mutuo e le relative approvazioni o rifiuti.",
    "crumbs": [
      "Syllabus",
      "R",
      "Modelli per probabilità"
    ]
  },
  {
    "objectID": "probmodel_app.html#caricamento-del-dataset",
    "href": "probmodel_app.html#caricamento-del-dataset",
    "title": "Probit e Logit",
    "section": "Caricamento del Dataset",
    "text": "Caricamento del Dataset\nPer iniziare, carichiamo il dataset utilizzando il pacchetto Ecdat e esaminiamo le prime righe dei dati.\n\nlibrary(Ecdat)\nlibrary(dplyr)\nlibrary(fixest)\ndata(Hmda)\nhead(Hmda)\n\n    dir   hir       lvr ccs mcs pbcr dmi self single uria condominium black\n1 0.221 0.221 0.8000000   5   2   no  no   no     no  3.9           0    no\n2 0.265 0.265 0.9218750   2   2   no  no   no    yes  3.2           0    no\n3 0.372 0.248 0.9203980   1   2   no  no   no     no  3.2           0    no\n4 0.320 0.250 0.8604651   1   2   no  no   no     no  4.3           0    no\n5 0.360 0.350 0.6000000   1   1   no  no   no     no  3.2           0    no\n6 0.240 0.170 0.5105263   1   1   no  no   no     no  3.9           0    no\n  deny\n1   no\n2   no\n3   no\n4   no\n5   no\n6   no\n\n\nIl dataset Hmda contiene 2381 osservazioni e 13 variabili. Le variabili sono descritte nella ?@tbl-descr.\n\n\n\n\n\n\n\nVariabile\nDescrizione\n\n\n\n\ndir\nRapporto tra i pagamenti del debito e il reddito totale. Indica quanto del reddito totale viene destinato al pagamento dei debiti.\n\n\nhir\nRapporto tra le spese abitative e il reddito. Misura la percentuale del reddito spesa per le abitazioni.\n\n\nlvr\nRapporto tra l’importo del prestito e il valore stimato della proprietà. Indica quanta parte del valore della proprietà è finanziata tramite prestito.\n\n\nccs\nPunteggio di credito al consumo, da 1 a 6, dove un valore basso rappresenta un buon punteggio.\n\n\nmcs\nPunteggio di credito ipotecario, da 1 a 4, dove un valore basso indica un buon punteggio di credito.\n\n\npbcr\nPresenza di cattivi precedenti creditizi pubblici. Variabile binaria (si/no) che indica se esistono precedenti negativi.\n\n\ndmi\nRichiesta di assicurazione sul mutuo negata. Variabile binaria (si/no) che indica se è stata negata l’assicurazione sul mutuo.\n\n\nself\nSe l’individuo è un lavoratore autonomo. Variabile binaria (si/no).\n\n\nsingle\nSe l’applicante è single. Variabile binaria (si/no) che indica lo stato civile.\n\n\nuria\nTasso di disoccupazione del 1989 nel Massachusetts, nel settore di impiego dell’applicante. Fornisce contesto economico.\n\n\ncondominium\nSe l’unità abitativa è un condominio (=1). Variabile binaria (1/0) che descrive il tipo di proprietà abitativa.\n\n\nblack\nSe l’applicante è di etnia afroamericana. Variabile binaria (si/no), usata in analisi di discriminazione razziale.\n\n\ndeny\nSe la domanda di mutuo è stata negata. Variabile binaria (si/no) che indica l’esito della richiesta di mutuo.",
    "crumbs": [
      "Syllabus",
      "R",
      "Modelli per probabilità"
    ]
  },
  {
    "objectID": "probmodel_app.html#statistiche-descrittive",
    "href": "probmodel_app.html#statistiche-descrittive",
    "title": "Probit e Logit",
    "section": "Statistiche Descrittive",
    "text": "Statistiche Descrittive\nIl dataset contiene un’osservazione con alcuni dati mancanti (NA). Questa osservazione può essere eliminata mediante na.omit. Questa funzione restituisce un data.frame con le osservazioni complete, cioè tutte le osservazioni per cui tutte le varabile sono non NA.\n\nHmda &lt;- na.omit(Hmda)\n\nIl pacchetto gtsummary contiene delle funzioni molto utili per calcolare le statistiche descrittive delle variabili presenti in un data.frame.\n\nlibrary(gtsummary)\nlibrary(dplyr)\nHmda &lt;- Hmda |&gt; select(deny, everything())\nHmda |&gt;\n  tbl_summary(\n    by = black,\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} ({p}%)\"\n    ),\n    digits = all_continuous() ~ 2,\n    label = deny ~ \"Mutuo Concesso (deny)\",\n    include = names(Hmda)\n  ) |&gt; \n  add_overall() |&gt;\n   modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Black**\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall, N = 2,3801\nBlack\n\n\nno, N = 2,0411\nyes, N = 3391\n\n\n\n\nMutuo Concesso (deny)\n285 (12%)\n189 (9.3%)\n96 (28%)\n\n\ndir\n0.33 (0.11)\n0.33 (0.11)\n0.35 (0.09)\n\n\nhir\n0.26 (0.10)\n0.25 (0.10)\n0.27 (0.08)\n\n\nlvr\n0.74 (0.18)\n0.73 (0.18)\n0.81 (0.16)\n\n\nccs\n\n\n\n\n\n\n\n\n    1\n1,353 (57%)\n1,226 (60%)\n127 (37%)\n\n\n    2\n441 (19%)\n390 (19%)\n51 (15%)\n\n\n    3\n126 (5.3%)\n99 (4.9%)\n27 (8.0%)\n\n\n    4\n77 (3.2%)\n53 (2.6%)\n24 (7.1%)\n\n\n    5\n182 (7.6%)\n140 (6.9%)\n42 (12%)\n\n\n    6\n201 (8.4%)\n133 (6.5%)\n68 (20%)\n\n\nmcs\n\n\n\n\n\n\n\n\n    1\n747 (31%)\n697 (34%)\n50 (15%)\n\n\n    2\n1,571 (66%)\n1,288 (63%)\n283 (83%)\n\n\n    3\n41 (1.7%)\n38 (1.9%)\n3 (0.9%)\n\n\n    4\n21 (0.9%)\n18 (0.9%)\n3 (0.9%)\n\n\npbcr\n175 (7.4%)\n115 (5.6%)\n60 (18%)\n\n\ndmi\n48 (2.0%)\n31 (1.5%)\n17 (5.0%)\n\n\nself\n277 (12%)\n252 (12%)\n25 (7.4%)\n\n\nsingle\n936 (39%)\n761 (37%)\n175 (52%)\n\n\nuria\n3.77 (2.03)\n3.83 (2.10)\n3.45 (1.50)\n\n\ncondominium\n686 (29%)\n519 (25%)\n167 (49%)\n\n\n\n1 n (%); Mean (SD)",
    "crumbs": [
      "Syllabus",
      "R",
      "Modelli per probabilità"
    ]
  },
  {
    "objectID": "probmodel_app.html#categorizzazione-del-rapporto-loan-to-value",
    "href": "probmodel_app.html#categorizzazione-del-rapporto-loan-to-value",
    "title": "Probit e Logit",
    "section": "Categorizzazione del rapporto loan-to-value",
    "text": "Categorizzazione del rapporto loan-to-value\nClassifichiamo il rapporto loan-to-value (lvr) in categorie basse, medie e alte per analisi future.\n\nHmda &lt;- Hmda |&gt; \n  mutate(lvra = case_when(\n    lvr &lt; 0.8 ~ \"low\",\n    lvr &gt; 0.95 ~ \"high\",\n    TRUE ~ \"medium\"\n  ))\n\nCreiamo la variable dummy denied che prende il valore 1 quando deny==\"yes\" and il valore 0 quando deny==\"no\".\n\nHmda &lt;- Hmda |&gt;\n  mutate(denied = case_when(\n    deny == \"yes\" ~ 1,\n    deny == \"no\" ~ 0\n  ))",
    "crumbs": [
      "Syllabus",
      "R",
      "Modelli per probabilità"
    ]
  },
  {
    "objectID": "probmodel_app.html#modello-di-probabilità-lineare",
    "href": "probmodel_app.html#modello-di-probabilità-lineare",
    "title": "Probit e Logit",
    "section": "Modello di Probabilità Lineare",
    "text": "Modello di Probabilità Lineare\nUtilizziamo un modello di regressione lineare per stimare la probabilità di rifiuto del mutuo.\n\nlibrary(fixest)\nlpm_HMDA &lt;-\n  feols(denied ~ black + dir + hir + lvra + ccs + mcs + pbcr + dmi \n        + self + uria + condominium,\n    data = Hmda\n  )\nlpm_HMDA\n\nOLS estimation, Dep. Var.: denied\nObservations: 2,380 \nStandard-errors: IID \n             Estimate Std. Error   t value   Pr(&gt;|t|)    \n(Intercept) -0.010389   0.044382 -0.234072 8.1495e-01    \nblackyes     0.084833   0.017443  4.863364 1.2298e-06 ***\ndir          0.444625   0.086863  5.118693 3.3242e-07 ***\nhir         -0.046887   0.096121 -0.487795 6.2574e-01    \nlvralow     -0.188589   0.033249 -5.672020 1.5832e-08 ***\nlvramedium  -0.157255   0.033323 -4.719096 2.5070e-06 ***\nccs          0.030792   0.003684  8.357460  &lt; 2.2e-16 ***\nmcs          0.019743   0.011090  1.780232 7.5166e-02 .  \npbcryes      0.196684   0.023159  8.492874  &lt; 2.2e-16 ***\ndmiyes       0.701116   0.041172 17.028780  &lt; 2.2e-16 ***\nselfyes      0.055638   0.018169  3.062313 2.2210e-03 ** \nuria         0.004864   0.002872  1.693364 9.0518e-02 .  \ncondominium  0.003865   0.012980  0.297765 7.6591e-01    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.277932   Adj. R2: 0.263457\n\n\nIl coefficient sulla variable blackyes (la dummy automaticamente creata da R che prende il valore 1 se black==\"yes\") è 0.085. L’interpretazione di questo coefficiente è che una persona di coloro a parità di altre caratteristiche ha una probabilità di avere il mutuo concesso più bassa di 0.085.\nPer avere un’idea di quanto sia grande questa differenza, possiamo calcolare la variazione relative della probabilità: \\[\n\\frac{\\Pr(black=1|X) - \\Pr(black=0|X)}{\\Pr(black=0)} = \\frac{0.085}{0.093} \\approx 0.91 (91\\%)\n\\] In altre parole, se rapportata alla probabilità (non condizionata) che hanno le persone non di colore di ottenere il mutuo (9.3%), le persone di colore hanno una probabilita’ piu bassa di ottenere il mutuo di circa il 90%.",
    "crumbs": [
      "Syllabus",
      "R",
      "Modelli per probabilità"
    ]
  },
  {
    "objectID": "probmodel_app.html#modello-logit-e-probit",
    "href": "probmodel_app.html#modello-logit-e-probit",
    "title": "Probit e Logit",
    "section": "Modello Logit e Probit",
    "text": "Modello Logit e Probit\n\nlogit_HMDA &lt;-\n  feglm(\n    denied ~ black + dir + hir + lvra + ccs + mcs + pbcr +\n      dmi + self + uria + condominium,\n    data = Hmda,\n    family = binomial(\"logit\")\n  )\n\n\nprobit_HMDA &lt;-\n  feglm(\n    denied ~ black + dir + hir + lvra + ccs + mcs + pbcr +\n      dmi + self + uria + condominium,\n    data = Hmda,\n    family = binomial(\"probit\")\n  )\n\nConfrontiamo i tre diversi modelli:\n\n\n\nMentre il coefficiente del modello lineare per la probabilità si presta immediatamente ad una interpretazione, i coefficienti del modello logit e probit ci danno esclusiavamente delle informazioni riguardo al segno della relazione fra la probabilità e le rispettive variabili. Per ottenere un’interpretazione bisogna procedere calcolando le predizioni.",
    "crumbs": [
      "Syllabus",
      "R",
      "Modelli per probabilità"
    ]
  },
  {
    "objectID": "probmodel_app.html#predizioni",
    "href": "probmodel_app.html#predizioni",
    "title": "Probit e Logit",
    "section": "Predizioni",
    "text": "Predizioni\nIl modo più semplice per ottenere una stima della differenza nella probabilita’ che il mutuo venga accettto per le persone di colore e quelle non di colore e’ procedere a calcolare le predizioni a dei valori prespecificati delle \\(X\\):\nPer il modello probit: \\[\n\\begin{aligned}\n\\underset{\\text{probabilità diniego mutuo per black}}{\\underbrace{\\Phi\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1}+\\hat{\\beta}_{2}\\bar{dir}+\\hat{\\beta}_{3}\\bar{hir}+\\hat{\\beta}_{6}\\bar{ccs}+\\hat{\\beta}_{7}\\bar{mcs}+\\hat{\\beta}_{11}\\bar{uria}\\right)}}\\\\-\\underset{\\text{probabilità diniego mutuo per non-black}}{\\underbrace{\\Phi\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{2}\\bar{dir}+\\hat{\\beta}_{3}\\bar{hir}+\\hat{\\beta}_{6}\\bar{ccs}+\\hat{\\beta}_{7}\\bar{mcs}+\\hat{\\beta}_{11}\\bar{uria}\\right)}}\n\\end{aligned}\n\\] e per il modello logit \\[\n\\begin{aligned}\n\\underset{\\text{probabilità diniego mutuo per black}}{\\underbrace{F\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1}+\\hat{\\beta}_{2}\\bar{dir}+\\hat{\\beta}_{3}\\bar{hir}+\\hat{\\beta}_{6}\\bar{ccs}+\\hat{\\beta}_{7}\\bar{mcs}+\\hat{\\beta}_{11}\\bar{uria}\\right)}}\\\\-\\underset{\\text{probabilità diniego mutuo per non-black}}{\\underbrace{F\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{2}\\bar{dir}+\\hat{\\beta}_{3}\\bar{hir}+\\hat{\\beta}_{6}\\bar{ccs}+\\hat{\\beta}_{7}\\bar{mcs}+\\hat{\\beta}_{11}\\bar{uria}\\right)}},\n\\end{aligned}\n\\] dove \\(F(\\cdot)\\) è la funzione di ripartizaione della distribuzione logistica.\nPer ottenere le predizioni è necessario creare un data.frame contenente i valori per le \\(X\\):\n\nnew &lt;- data.frame(\n  \"dir\" = mean(Hmda$dir),\n  \"hir\" = mean(Hmda$hir),\n  \"lvra\" = \"low\",\n  \"ccs\" = mean(Hmda$ccs),\n  \"mcs\" = mean(Hmda$mcs),\n  \"pbcr\" = \"no\",\n  \"dmi\" = \"no\",\n  \"self\" = \"no\",\n  \"black\" = c(\"no\", \"yes\"),\n  \"uria\" = mean(Hmda$uria),\n  \"condominium\" = 0\n)\n\nPossiamo ottenere le predizioni per i due modelli usando la funzione predict:\n\nlogit_pred &lt;- predict(logit_HMDA, newdata = new)\nprobit_pred &lt;- predict(probit_HMDA, newdata = new)\n\nlogit_pred e probit_pred sono due vettori che hanno come prime elemento la stima della probabilità per black and come secondo la stima della probabilità per non-black (condizionatamente ai valori specificati in new per le altre variabili). La differenza di questi due valori è la stima della differenza della probabilita’.\n\ndelta_logit = diff(logit_pred)\ndelta_logit\n\n[1] 0.04204562\n\n\n\ndelta_probit = diff(probit_pred)\ndelta_probit\n\n[1] 0.05186245\n\n\nLo stesso risultato puo’ essere ottenuto usando marginaleffects\n\nlibrary(marginaleffects)\n\navg_slopes(logit_HMDA, newdata = \"mean\", variables = \"black\")\n\n\n  Term Contrast Estimate Std. Error   z Pr(&gt;|z|)   S  2.5 % 97.5 %\n black yes - no    0.042     0.0145 2.9  0.00375 8.1 0.0136 0.0705\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\n\navg_slopes(probit_HMDA, newdata = \"mean\", variables = \"black\")\n\n\n  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n black yes - no   0.0519      0.017 3.06  0.00222 8.8 0.0186 0.0851\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nAlternativamente, possiamo calcolare la media delle probabilità\n\nHmda_tmp &lt;- Hmda\nHmda_tmp$black &lt;- \"yes\"\nproba_black &lt;- predict(logit_HMDA, newdata = Hmda_tmp)\nHmda_tmp$black &lt;- \"no\"\nproba_nonblack &lt;- predict(logit_HMDA, newdata = Hmda_tmp)\n\nmean(proba_black - proba_nonblack)\n\n[1] 0.06281968\n\n\nQuesta stima e’ equivalente a quella ottenuta con marginaleffects\n\navg_slopes(logit_HMDA, variables = \"black\")\n\n\n  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S  2.5 % 97.5 %\n black yes - no   0.0628     0.0184 3.41   &lt;0.001 10.6 0.0267 0.0989\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nI vantaggi di utilizzare marginaleffects rispetto a procedere ai calcoli manuali sono che 1) otteniamo anche gli errori standard per l’effetto e 2) marinaleffects calcola gli effetti per tutte le variabili del modello:\n\navg_slopes(logit_HMDA)\n\n\n        Term      Contrast Estimate Std. Error       z Pr(&gt;|z|)    S     2.5 %\n black       yes - no       0.06282    0.01841  3.4131  &lt; 0.001 10.6  0.026745\n ccs         dY/dX          0.02195    0.00299  7.3335  &lt; 0.001 42.0  0.016084\n condominium 1 - 0          0.00251    0.01262  0.1989  0.84235  0.2 -0.022227\n dir         dY/dX          0.35585    0.07822  4.5495  &lt; 0.001 17.5  0.202548\n dmi         yes - no       0.73198    0.06885 10.6314  &lt; 0.001 85.3  0.597038\n hir         dY/dX         -0.00628    0.09347 -0.0672  0.94640  0.1 -0.189488\n lvra        low - high    -0.15213    0.04388 -3.4667  &lt; 0.001 10.9 -0.238139\n lvra        medium - high -0.11724    0.04381 -2.6763  0.00744  7.1 -0.203099\n mcs         dY/dX          0.01947    0.01057  1.8415  0.06555  3.9 -0.001252\n pbcr        yes - no       0.12664    0.02810  4.5064  &lt; 0.001 17.2  0.071563\n self        yes - no       0.05348    0.02085  2.5651  0.01031  6.6  0.012618\n uria        dY/dX          0.00474    0.00257  1.8457  0.06493  3.9 -0.000293\n   97.5 %\n  0.09889\n  0.02782\n  0.02725\n  0.50915\n  0.86693\n  0.17692\n -0.06612\n -0.03138\n  0.04019\n  0.18172\n  0.09435\n  0.00977\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nQueste stime sono stime medie. Possiamo anche visualizzare le stime usilizzando le \\(X\\) sdi ciascun individuo nel nostro campione e poi visualizzare le differenze.\n\na &lt;- slopes(logit_HMDA)\nb &lt;- slopes(logit_HMDA, newdata = \"mean\")\nlibrary(ggplot2)\nggplot(a, aes(x=estimate)) + \n  geom_histogram(bins = 60) + \n  geom_vline(data=b, aes(xintercept = estimate), color = \"darkred\") + \n  facet_wrap(contrast~term, scales= \"free\") + \n  theme_minimal()",
    "crumbs": [
      "Syllabus",
      "R",
      "Modelli per probabilità"
    ]
  }
]